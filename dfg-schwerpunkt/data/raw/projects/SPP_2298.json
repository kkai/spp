{
  "spp_number": "SPP 2298",
  "spp_title": "Theoretische Grundlagen von Deep Learning",
  "spp_url": "https://gepris.dfg.de/gepris/projekt/441826958",
  "projects_count": 33,
  "projects": [
    {
      "project_id": "463293876",
      "url": "https://gepris.dfg.de/gepris/projekt/463293876",
      "title": "Adaptive Neuronale Tensor Netzwerke für parametrische Partielle Differentialgleichungen",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463293876",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Der Schwerpunkt des Projekts liegt auf der zertifizierten adaptiven Darstellung von hochdimensionalen parametrischen partiellen Differentialgleichungslösungen in einer adäquaten modell-agnostischen tiefen Neuronalen Netzwerkarchitektur (NN), die durch hierarchische Tensornetzwerke unterstützt wird. Aus vorhandenen Expressivitätsergebnissen wissen wir, dass solche Gleichungen im Prinzip eine NN-Darstellung der Lösung ermöglichen würden, aber der Engpass liegt in der Definition der Netzwerktopologie, der Anpassung an die Lösung und dem Finden der Darstellungsparameter während des Trainings. Ziel des Projekts ist es, die theoretischen und praktischen Grundlagen für adaptive und konvergierende NN-Näherungen parametrischer partieller Differentialgleichungen sowohl für Vorwärts- als auch für Inverse Probleme zu entwickeln. Die beiden Grundpfeiler des Fundaments sind 1) ein zuverlässiger und berechenbarer a posteriori Fehlerschätzer, der zu einem konvergenten Algorithmus für die (tiefe) NN-Approximation führt, und2) ein zuverlässiger arithmetischer Rahmen für das Finden der Darstellungsparameter der (tiefen) NN-Approximation mit Hilfe lokaler hierarchischer Tensornetzdarstellungen.Unsere Vision ist es, die tiefen NN zu einem zuverlässigen und effizienten Berechnungsansatz für diese Quantifizierungen zu machen, der schließlich die derzeitigen Best-of-Class-Methoden übertrifft und zu einem vielseitigen Werkzeug für ansonsten unlösbare Probleme wird.  Darüber hinaus trägt das Projekt zu einem besseren Verständnis und einer besseren Nutzung der Beziehung zwischen tiefen Netzwerkdarstellungen und hierarchischen Tensornetzen bei.  Die einfachere multilineare Struktur von Tensornetzwerken wird uns helfen, den Weg für die hochgradig nichtlineare Struktur von tiefen NN zu erkunden.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464110610",
      "url": "https://gepris.dfg.de/gepris/projekt/464110610",
      "title": "Aspekte der Statistischen Analyse von Trainingstrajektorien von tiefen neuronalen Netzen",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2026",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464110610",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Tiefe neuronale Netze (DNNs) gehören in vielen Anwendungsbereichen zu den erfolgreichsten Methoden des maschinellen Lernens. Trotz dieses Erfolgs und der Tatsache, dass diese Methoden seit nunmehr circa 40 Jahren betrachtet werden, ist unser statistisches Verständnis ihrer Lernmechanismen noch immer sehr begrenzt. Einer der Gründe für dieses mangelnde Verständnis ist die Tatsache, dass in vielen Fällen die Instrumente der klassischen statistischen Lerntheorie nicht mehr angewendet werden können. Das Gesamtziel dieses Projekts ist es, Schlüsselaspekte einer statistischen Analyse von praxisnahen DNN-Trainingsalgorithmen zu untersuchen. Insbesondere werden wir die statistischen Eigenschaften von Trajektorien untersuchen, die durch (Varianten des) Gradientenabstieg(s) erzeugt werden. Hierbei liegt unser Schwerpunkt auf der Frage, ob solche Trajektorien Prädiktoren enthalten, die gute Lerngarantien im Sinne von hoher Generalisierungsfähigkeit besitzen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464101476",
      "url": "https://gepris.dfg.de/gepris/projekt/464101476",
      "title": "Auf dem Weg zu einer überall verlässlichen Klassifizierung - Ein gemeinsamer Rahmen für Robustheit gegen böswillige Attacken und die Erkennung von Out-of-Distribution Eingaben",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2021 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464101476",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Robustheit gegen böswillige Attacken und die Detektion von Out-of-Distribution Eingaben wurden bis jetzt separat behandelt. Die Trennung dieser Probleme ist jedoch aus unserer Sicht künstlich, da sie inhärent miteinander verbunden sind. Eine Verallgemeinerung der Robustheit gegen böswillige Attacken über die beim Training verwendeten Bedrohungsmodelle hinaus scheint nur möglich indem man das Adversarial Training von Madry et al. durch einen gemeinsamen Rahmen für die Robustheit gegen böswillige Attacken und die Detektion von Out-of-Distribution Eingaben ersetzt.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "543964668",
      "url": "https://gepris.dfg.de/gepris/projekt/543964668",
      "title": "Beschleunigung von Diffussionsmodellen durch dünnbesetzte neuronale Netze",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 543964668",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Mathematische Konvergenzgarantien für Diffusionsmodelle sind in letzter Zeit zu einem aktiven Forschungsthema geworden. Studien haben nicht-asymptotische Konvergenzraten für die Variation zwischen generierten und ursprünglichen Stichproben bewiesen. Diese Raten wachsen mit O(d/epsilon), das heißt, sie haben eine klare polynomielle Abhängigkeit von der Dimensionalität der Daten \"d\" und dem Fehlerniveau \"epsilon\". Unser Ziel ist es, diese Konvergenzraten in Bezug auf \"d\" zu verbessern. Wir versuchen Raten im Wesentlichen von der Größenordnung O(s/ε) zu entwickeln, wobei die effektive Dimension \"s\" viel kleiner als \"d\" ist. Zum Beispiel kann \"s\" die Dünnbesetztheit des Schätzers abbilden. Um diese Raten zu erreichen, entwickeln wir zwei unterschiedliche Ansätze: Erstens betrachten wir Zielverteilungen, die auf einer niedrigdimensionalen Mannigfaltigkeit liegen, die wiederum in einen hochdimensionalen Umgebungsraum eingebettet ist. In diesem Fall nutzen wir die geschätzte Score-Funktion, insbesondere die in ihr gespeicherten Informationen über die Mannigfaltigkeit, um die Stichprobenverfahren zu beschleunigen. Zweitens betrachten wir Zielverteilungen deren Score-Funktion durch ein dünnbesetztes neuronales Netz approximiert werden können. In diesem Fall verwenden wir Regularisierungstechniken aus der hochdimensionalen Statistik, um die Stichprobenverfahren zu beschleunigen. Nachdem wir diese mathematischen Ergebnisse hergeleitet haben, verbinden wir sie mit den algorithmischen Herausforderungen, die bei der Minimierung von Score-Matching-Funktionen auftreten.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "463892539",
      "url": "https://gepris.dfg.de/gepris/projekt/463892539",
      "title": "Beweisbare Robustheitszertifizierung von Neuronalen Netzen für Graphen",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2021 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463892539",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Neuronale Netze für Graphen (GNNs) haben sich neben CNNs und RNNs zu einem essentiellen Baustein moderner Deep Learning Modelle entwickelt, mit weitreichender Bedeutung für Domänen wie der Chemie (Molekulargraphen), den Sozialwissenschaften (soziale Netzwerke), der Biomedizin (Genregulationsnetzwerke) und vielen mehr. Aktuelle Studien zeigen jedoch, dass GNNs nicht robust sind: selbst kleine Perturbationen der Graphstruktur oder Attributinformation können die Vorhersagen der Modelle dramatisch fehlleiten. Eine Nutzung in realen Anwendungen wird dadurch nahezu unmöglich, da Daten häufig verunreinigt, verrauscht, oder sogar gezielt manipuliert sind.Das Ziel des Projektes ist es das Vertrauen in GNNs zu erhöhen indem Methoden für Ihre Robustheitszertifizierung erforscht werden, d.h. die mathematisch nachweisbare Aussagen treffen, ob Perturbationen einer bestimmte Klasse zu keiner Veränderung in der Vorhersage führen. Damit wird gezielt eine Kernsäule des Schwerpunktprogramms, das Thema “safety and robustness”, adressiert. Wir untersuchen hierbei neue theoretische Garantien für wichtige maschinelle Lernaufgaben auf Knoten- und Graphebene, welche weit über die wenigen existierenden Verfahren hinausgehen sowie deren kritischen Einschränkungen adressieren. Insbesondere erforschen wir Zertifizierungsprinzipien welche fundamentale, jedoch bislang nicht berücksichtigte, Eigenschaften von Lernaufgaben auf Graphen, wie die simultane Vorhersage mehrerer Ausgaben oder die Permutationsinvarianz, berücksichtigen. Die Zertifikate sollen in all diesen Fällen durch Ausnutzung weiterer modellspezifischer Informationen verbessert werden. Da GNNs auch insbesondere für die Regression von kontinuierlichen Zielvariablen, z.B. der Vorhersage von Energiepotentialen von Molekülen, eingesetzt werden, untersuchen wir zusätzlich und erstmalig auch Zertifikate für dieses herausfordernde Szenario. Hand in Hand mit der Erforschung neuer Zertifikate planen wir ebenso die Untersuchung ihrer inhärenten Einschränkungen, verursacht z.B. durch den Fluch der Dimensionalität oder durch Relaxierung der zugrunde liegenden schweren Optimierungsprobleme.Insgesamt liefert das Projekt durch die Herleitung formaler Robustheitsgarantien einen wesentlichen Baustein für die theoretischen Grundlagen von GNNs. Jedes Arbeitspaket geht hierbei auf unterschiedliche und notwendige Aspekte der Robustheit ein. Durch die Vielzahl von Anwendungsszenarien von GNNs erwarten wir eine breite Wirkung hinein in Forschung und Anwendung.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "543917411",
      "url": "https://gepris.dfg.de/gepris/projekt/543917411",
      "title": "Deep Learning aus der Perspektive von Wahrscheinlichkeit und Geometrie",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable ComputingTheoretische Informatik",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 543917411",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Laplace-Approximationen haben sich als ein mächtiges und effizientes Werkzeug für Deep Learning neu etabliert. Sie stützen sich auf die starken Werkzeuge der automatischen Differenzierung und numerische lineare Algebra um neue Funktionalität zu ermöglichen, die aufgrund ihrer hohen Rechenkosten zuvor zu Nischendasein verdammt waren. Insbesondere liefern Laplace-Approximationen analytische Bayesianische Formalismen für Deep Learning, und verwandeln jedes tiefe neuronale Netzwerk in einen approximativen Gaußschen Prozess. Gleichzeitig definieren sie auch eine Metrik, mit einer zugehörigen Riemannschen Mannigfaltigkeit für das tiefe Netz und seine Parameter. Dieser Antrag an das SPP sucht die jüngsten Ergebnisse sowohl in theoretischer als auch in algorithmischer Richtung zu erweitern. Auf der theoretischen Seite zielt das Projekt darauf ab, die Riemannsche Geometrie von Trainingszielen besser zu charakterisieren, und damit sowohl das Bayesianische Verständnis als auch das Training tiefer Netze zu verbessern. Als direktes Ergebnis wird das Projekt dann neue Algorithmen und funktionale Erweiterungen des Deep Learning durch Reparameterisierung entwickeln, um bessere Unsicherheiten für tiefe Netze zu ermöglichen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464101359",
      "url": "https://gepris.dfg.de/gepris/projekt/464101359",
      "title": "Deep-Learning basierte Regularisierung inverser Probleme",
      "investigators": "Professor Dr. Martin  Burger;Professorin Dr. Gitta  Kutyniok",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464101359",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Deep Learning wurde in den letzten Jahren sehr populär in Gebieten wie Bildverarbeitung und ist deshalb auch als Methode zur Regularisierung von inversen Problemen von wachsendem Interesse. Neben ihrem großen Potential sind die Entwicklung und vor allem das Verständnis tiefer Netzwerke in diesem Gebiet noch in ihren Kinderschuhen. Deshalb sollen in diesem Projekt die Konstruktion von Regularisierungsmethoden für schlechtgestellte inverse Probleme basierend auf Deep Learning und ihre theoretischen Grundlagen untersucht werden. Besondere Ziele sind die Entwicklung robuster und interpretierbarer Resulate, die zuerst die Entwicklung neuer Konzepte von Robustheit und Interpretierbarkeit in diesem Zusammenhang nötig machen.  Die theoretischen Entwicklungen werden durch umfangreiche numerische Studien begleitet, dazu werden Maße und Benchmark Probleme für faire Vergleiche verschiedener Ansätze entwickelt.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464123384",
      "url": "https://gepris.dfg.de/gepris/projekt/464123384",
      "title": "Deep Learning für nicht-lokale partielle Differentialgleichungen",
      "investigators": "Professor Dr. Arnulf  Jentzen;Professorin Dr. Gitta  Kutyniok, seit 10/2022",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464123384",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Partielle Differentialgleichungen (PDEs) gehören zu den allgegenwärtigen Instrumenten um Probleme aus Natur, Technik und von Menschen geschaffenen Systemen zu beschreiben. Insbesondere tauchen PDEs auf in der Bewertung und Absicherung von Finanzderivaten oder in der Modellierung von Biodiversität um ein besseres Verständnis von Ökosystemen  unter Klimaveränderungen zu gewinnen. Die PDEs, die in den oben genannten Anwendungen auftreten, sind häufig nicht-lokale nicht-lineare hoch-dimensionale PDEs, wobei die Dimension  der PDE z.B. im Falle der Bewertung von Derivaten gerade der Anzahl der Basiswerte entspricht (z.B. Aktien, Rohstoffe, Währungen) und im Falle von Biodiversitätsmodellen die Dimension grob gesprochen der Anzahl der Merkmale der Individuen im betrachteten Ökosystem entspricht. Realistische PDE-basierte Modelle in diesen Anwendungsbereichen sind oft nicht-lokal aufgrund des möglichen Sprungverhaltens der Preise der Basiswerte im Falle von Finanzderivaten und im Falle von Biodiversitätsmodellen aufgrund der Tatsache, dass jedes Individuum sich in Konkurrenz mit Individuen mit allen möglichen Merkmalen befindet. Es ist fast immer unmöglich nicht-lokale nicht-lineare PDEs explizit zu lösen und es gibt auch  bis heute keine numerische Methode, mit der man diese hoch-dimensionalen PDEs effizient lösen kann. Der Hauptbeitrag dieses Projekts besteht darin, dass Deep Learning (DL)-basierte Methoden vorgeschlagen und untersucht werden, die zur approximativen Lösung solcher PDEs eingesetzt werden können. Das Projekt wird die gesamte Bandbreite von numerischen Simulationen in den konkreten oben genannten Anwendungen bis zu rigorosen mathematischen Konvergenzresultaten abdecken. Insbesondere wird rigoros bewiesen, dass die tiefen neuralen Netze in den vorgeschlagenen DL Algorithmen tatsächlich die grundlegende Fähigkeit haben den Fluch der Dimension zu überwinden in der numerischen Approximation solcher nicht-lokalen nicht-linearen PDEs im Sinne, dass die Anzahl der Parameter in den neuralen Netzen höchstens polynomial wächst sowohl im Reziproken der vorgeschriebenen Approximationsgenauigkeit und der Dimension der PDE. Wir beabsichtigen auch eine vollständige Fehleranalyse für den vorgeschlagenen DL Algorithmus durchzuführen und damit Konvergenz für den Approximations-, den Generalisierungs- und den Optimierungsfehler sicherzustellen, was insbesondere auch Konvergenz des vorgeschlagenen Algorithmus beweist. In bestimmten Situation erlauben die neu entwickelten Methoden auch vollständig Dimensions-unabhängige Konvergenzraten herzuleiten und damit zu beweisen, dass DL Algorithmen in der Lage sind, den Fluch der Dimension bei der approximativen Lösung dieser PDEs zu überwinden. Wir erwarten, dass die Techniken zur Fehleranalyse, die in diesem Projekt entwickelt werden, auch für die mathematische Fehleranalyse von allgemeineren DL-basierten Algorithmen für andere Probleme, weit über PDE-Probleme hinaus, relevant sein werden.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep LearningEhemaliger AntragstellerProfessor Dr. Lukas  Gonon, bis 9/2022"
    },
    {
      "project_id": "463409137",
      "url": "https://gepris.dfg.de/gepris/projekt/463409137",
      "title": "Generative Modelle für Bayesche Inverse Probleme in der Bildverarbeitung",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463409137",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Mit diesem Folgeantrag möchten wir unsere Arbeit zu verallgemeinerten Normalizing Flows und neuronalen Gradientenflüsse im SPP fortsetzen. Basierend auf unseren bisherigen Ergebnissen wollen wir die Approximation von Posteriorverteilungen gut gestellter Bayesescher inverser Probleme durch bedingte Normalizing Flows untersuchen. Dabei sollen Ergebnisse zur Approximation glatterer Funktionen durch Normalizing Flows mit Kenntnissen über optimale Transportabbildungen gekoppelt werden. Bezüglich Gradientenflüssen werden wir drei Problemstellungen betrachten.  Erstens interessiert uns die Konvergenz von Wasserstein-Gradientenflüssen der Maximum Mean Discrepancy (MMD) mit Riesz-Kernen, die verschiedene vorteilhafte Eigenschaften haben. Da die entsprechenden Funktionale nicht lambda-konvex entlang von Geodäten in Wasserstein-2 Räumen sind, können entsprechende Methoden nicht unmittelbar verwendet werden. Wir wollen kritische Punkte solcher MMD-Funktionale bestimmen. Ferner sollen aktuelle Ergebnisse zu Wasserstein Gradientenflüssen von Coulomb-Potentialen einbezogen werden. Zweitens werden wir MMD-regulierte f-Divergenzen betrachten. Diese haben den Vorteil, dass im Gegensatz zu f-Divergenzen mit unendlicher Rezessionkonstante, Bedingungen an die absolute Stetigkeit der Maße wegfallen. Basierend auf dem Kern-Mean Einbettungsoperator von Maßen in Hilberträume mit reproduzierendem Kern (RKHS) werden wir solche regulierten Funktionale als Moreau Einhüllende in RKHS interpretieren und bekannten Eigenschaften von Letzteren bei Wasserstein-Gradientenflüssen ausnutzen. Drittens werden wir uns mit uni- und multivariaten Kernen befassen, die durch ein Projektionsverfahren in gleichverteilte Richtungen auseinander entstehen. Sogenannte sliced Kerne können verwendet werden, um das Lernen neuronaler Netzwerke zu beschleunigen. Während die Beziehung zwischen Riesz-Kernen klar ist, interessieren wir uns jetzt für integrale positiv definite Kerne, die z.B. in Stein-Gradientenflüssen der Kullback-Leibler-Divergenz eine Rolle spielen. Sobald die Beziehung geklärt ist, werden wir sliced Kerne auch in entsprechenden Gradientenflüssen verwenden. Schließlich möchten wir Algorithmen für den optimalen Transport unter zusätzlichen Nebenbedingungen entwerfen und analysieren. Dabei sind wir vor allem an Momentenbedingungen interessiert. Genauer interessieren wir uns für die Klasse der verallgemeinerten iterativen Skalierungsverfahren, auch bekannt als blockiterative simultane multiplikative algebraische Rekonstruktionstechnik oder als besondere Form des Mirror Descent. Insbesondere hoffen wir einige seit langem offenen Konvergenzfragen zu Blockiterationen zumindest für unsere optimalen Transportprobleme zu beantworten.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "544579844",
      "url": "https://gepris.dfg.de/gepris/projekt/544579844",
      "title": "GeoMAR: Geometrische Methoden für Adversarial Robustness",
      "subject_area": "MathematikSicherheit und Verlässlichkeit, Betriebs-, Kommunikations- und verteilte Systeme",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 544579844",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Die Anfälligkeit von Deep Learning bezüglich feindlicher Angriffe und sonstiger Störungen stellt in realen Anwendungssituationen ein erhebliches Sicherheitsrisiko dar. Viele Ansätze zur Verbesserung der Robustheit neuronaler Netze sind lediglich heuristisch motiviert und das mathematische Verständnis ihrer Wirkungsweise ist kaum ausgebildet. Infolgedessen hat sich die Mehrzahl der in der Vergangenheit vorgeschlagenen Methoden zum Trainieren robuster neuronaler Netze als unwirksam erwiesen. Eine der wenigen wirklich robusten Ansätze ist Adversarial Training (AT), welches zudem durch eine in der Entwicklung begriffene mathematische Theorie gestützt wird. AT induziert jedoch einen Zielkonflikt zwischen Robustheit auf gestörten und Genauigkeit auf ungestörten Daten. Zusätzliche Datenmengen können hierbei zwar Abhilfe schaffen, allerdings sind derartig datenintensive Methoden sehr rechenintensiv und theoretisch schlecht verstanden. Im Rahmen des Projekts GeoMAR wollen wir eine mathematische Theorie für effektive und robuste Trainingsmethoden entwickeln. Abgesehen von theoretischen Erkenntnissen planen wir auch, das entwickelte Wissen auf praktische Algorithmen zu übertragen. Die Hauptziele von GeoMAR sind die Analyse von robuster Geometrie, die Abmilderung des Zielkonfliktes zwischen Genauigkeit und Robustheit, die Analyse und der Vergleich der geometrischen Eigenschaften von neuronalen Netzen durch neuartige Testzeitmethoden sowie die Skalierbarkeit auf große Datensätze. Um dies zu erreichen, werden wir Robustheit geometrisch interpretieren und sie als Regularitätseigenschaft der Entscheidungsgrenze eines neuronalen Netzes modellieren. Wir werden diesen Standpunkt nutzen, um neue robuste Trainingsmethoden zu entwickeln und sie mit maßgeschneiderten Optimierungsmethoden zu lösen. Die Robustheit dieser Ansätze wird mit Testzeitmethoden quantifiziert und wir werden unsere Algorithmen skalieren, indem wir generative Modelle zur Berechnung von Angriffen nutzen. Die angestrebten Projektergebnisse von GeoMAR sind geometrische, interpretierbare und skalierbare Trainingsmethoden mit nachweisbar besserer Genauigkeit und Robustheit. Somit wird das Projekt das mathematische Verständnis von Robustheit des maschinellen Lernens fördern und effiziente Algorithmen für das Training von Deep-Learning-Systemen für reale Anwendungen entwickeln.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep LearningInternationaler BezugKanada, USAKooperationspartnerProfessor Gauthier  Gidel, Ph.D.;Professor Ryan  Murray, Ph.D."
    },
    {
      "project_id": "463910157",
      "url": "https://gepris.dfg.de/gepris/projekt/463910157",
      "title": "Global optimales Training von Neuronalen Netzen",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463910157",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Das Trainieren von künstlichen neuronalen Netzen ist die zentrale Optimierungsaufgabe im Deep Learning und eines der wichtigsten Optimierungsprobleme des maschinellen Lernens. Aufgrund seiner inhärenten algorithmischen Komplexität, werden in der Regel lokale Methoden wie stochastische Subgradientenverfahren verwendet. Verschiedene Methoden und Initialisierungen weisen verschiedenes Verhalten in Bezug auf Generalisierbarkeit, Robustheit (in Bezug auf Rauschen oder feindliche Störungen) und Erklärbarkeit (Bedeutung) auf.Aufgrund der fundamentalen Bedeutung des Trainingsproblems für neuronale Netze ist daher wichtig und natürlich, Methoden zur Berechnung global optimaler Lösungen dieses Trainingsproblems und die Struktur seiner Lösungen zu untersuchen. Auf technischer Ebene erlaubt dies einen Vergleich mit den Ergebnissen der lokalen Verfahren auf kleinen Netzen.Kurz zusammengefasst sind unsere Ziele: Berechne und analysiere global optimale Lösungen des Trainingsproblems für neuronale Netze und untersuche seine Generalisierbarkeit, Erklärbarkeit und Robustheit.Um diese Ziele zu erreichen werden wir:1. Ganzzahlige Optimierungsmethoden einsetzen: Wir werden das Trainingsproblems mit Methoden der gemischt-ganzzahligen nichtlinearen Optimierung angehen. Wir haben insbesondere vor, existierende Lösungsmethoden, die auf räumlichen Branch-and-cut basieren, zu verbessern. Dies erfordert das Ausnutzen sowohl von Modell- als auch Netzstruktur.2. Symmetrie ausnutzen: Wir werden weiterhin mögliche Symmetrien der Daten und des Netzes ausnutzen, um den Lösungsprozess zu beschleunigen und den Rechenaufwand zu reduzieren, sowie eine symmetrische Lösung zu garantieren. Dies ist wichtig, wenn die zugrundeliegenden Probleme eine Symmetrie ausweisen die durch neuronale Netze ausgedrückt werden soll. Weiter muss Symmetriebehandlung in einen exakten Optimierungsansatz integriert werden, um die prinzipiellen Möglichkeiten und Beschränkungen beim Ausnutzen dieser Strukturen zu verstehen. Unser Ziel ist es Methoden zu entwickeln, die Symmetrien in einem allgmeinen Kontext automatisch behandelt, und die sich auf zukünftige bzw. andere Netzarchitekturen übertragen lassen.3. Dünnbesetztheit garantieren: Wir werden echte Dünnbesetztheit (in einem l0-Sinne) in neuronale Netze integrieren. Dünnbesetztheit wird oft heuristisch angegangen, z.B. durch iteratives Thresholding, was oft zu suboptimaler Dünnbesetztheit führt. Die Dünnbesetztheit zu optimieren bzw. zu beschränken kann jedoch direkt in das gemischt-ganzzahlige nichtlineare Framework integriert werden.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464121491",
      "url": "https://gepris.dfg.de/gepris/projekt/464121491",
      "title": "Impliziter Bias im adversariellen Training",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464121491",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Trotz aller Erfolge des Deep Learnings können trainierte tiefe neuronale Netze sehr empfindlich auf kleine Störungen reagieren. Dies macht Deep-Learning-Methoden anfällig für Angriffe. Mehrere Ansätze wurden vorgeschlagen, um neuronale Netze robuster zu machen. Ein besonders vielversprechender ist das adversarielle Lernen (adversarial learning), bei dem Worst-Case-Störungen während der Lernphase berücksichtigt werden. Das vorgeschlagene Projekt zielt darauf ab, solche Strategien in verschiedenen Kontexten zu untersuchen. Wir werden einen Schwerpunkt auf neuronale ODEs legen, die als eine unendlich tiefer Grenzwert von klassischen neuronalen Netzen angesehen werden können. In diesem Kontext werden wir adversariales Training untersuchen, indem wir es als Mean-Field-Minimax-Optimalsteuerungsproblem formulieren. Aufgrund des aktuellen Trends zu immer größeren Modellen des maschinellen Lernens übersteigt die Anzahl der Parameter des neuronalen Netzes in der Regel die Anzahl der Trainingsparameter bei Weitem. In diesem Szenario besitzt die empirische Verlustfunktion unendlich viele globale Minimierer und die verwendeten Lernalgorithmen - Varianten des (stochastischen) Gradientenabstiegs - bewirken einen impliziten Bias zu gewissen Lösungen. Überraschenderweise zeigen die gängigen Lernalgorithmen eine sehr gute Generalisierung auf ungesehene Daten. Erste theoretische Ergebnisse zu vereinfachten (linearen) Netzwerkmodellen deuten auf eine implizite Tendenz zu spärlichen (sparse) oder Lösungen niedrigen Ranges hin. Für adversarielles Lernen liegen jedoch bisher keine theoretischen (und größtenteils auch keine numerischen) Ergebnisse vor, und wir planen, diese Lücke im Rahmen dieses Projekts zu schließen. Ein neuerer Ansatz zur Verbesserung der Generalisierungseigenschaften von gelernten Netzwerken ist Sharpness-Aware-Minimization, welches darauf abzielt, flache Minimierer der Verlustfunktion zu bevorzugen. Wir werden den impliziten Bias dieses Ansatzes für verschiedene Netzwerkarchitekturen, einschließlich neuronaler ODEs, analysieren und Kombinationen mit adversarialem Lernen untersuchen. Darüber hinaus werden wir uns auch mit Transformer-Architekturen befassen, für die impliziter Bias und adversariales Lernen ebenfalls ein weitestgehend offenes Gebiet sind.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464109215",
      "url": "https://gepris.dfg.de/gepris/projekt/464109215",
      "title": "Kombinatorische und implizite Methoden für Deep Learning - Phase II",
      "subject_area": "MathematikBild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464109215",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Dieses Projekt entwickelt theoretische Grundlagen des Deep Learning mit Schwerpunkt auf kombinatorischen und algebraischen Strukturen im Lernen mit neuronalen Netzen. Wir schlagen Brücken zur algebraischen Statistik und zur tropischen Geometrie, um die Theorie auf drei verschiedenen Ebenen voranzutreiben: einzelne Funktionen, Funktionsklassen und Funktionsklassen zusammen mit einem Trainingsziel. Wir nutzen Techniken zur Kombinatorik von Anordnungen, algebraischen Implizitisierung und zur eingeschränkten Optimierung, um eine genaue Beschreibung endlicher Netzwerke mit expliziter Kontrolle über die Rolle der Daten zu ermöglichen. Eine wichtige Innovation, die wir fortsetzen, sind implizite Beschreibungen der Funktionsklassen, die durch neuronale Netze repräsentiert werden. Wir richten unsere Aufmerksamkeit auf neue Herausforderungen bei der Optimierung von leicht überparametrisierten Netzwerken, der Erforschung von Aktivierungsfunktionen, Netzwerken mit eingeschränkten Parametern, und der Entwicklung systematischer Ansätze zur Charakterisierung von Trainingsinvarianten.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "463889142",
      "url": "https://gepris.dfg.de/gepris/projekt/463889142",
      "title": "Koordinationsfonds",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463889142",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Das Schwerpunktprogramm (SPP) \"Theoretische Grundlagen von Deep Learning\" wird an der LMU München koordiniert. Dazu gehören die Organisation der verschiedenen geplanten Kooperations- und Netzwerkaktivitäten wie Jahreskonferenzen, Workshops, Minitutorials und Winterschulen, die Koordination der Auswahl und Einladung von PP-Gastprofessorinnen und -Gastprofessoren sowie PP-Fellows, die Einrichtung und Pflege einer Webseite sowie die Bereitstellung eines Newsletters und die Betreuung eines Preprint-Servers. Auch alle PR Aktivitäten bedürfen der Organisation. Darüber hinaus wollen wir den wissenschaftlichen Nachwuchs wie auch Forscherinnen durch verschiedene Maßnahmen fördern, die koordiniert werden müssen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464123524",
      "url": "https://gepris.dfg.de/gepris/projekt/464123524",
      "title": "Lösungsverfahren für lineare inverse Probleme basierend auf neuronalen Netzen: Generalisierung, Robustheit und Quantifizierung von Unsicherheiten",
      "subject_area": "Kommunikationstechnik und -netze, Hochfrequenztechnik und photonische Systeme, Signalverarbeitung und maschinelles Lernen für die InformationstechnikMathematik",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464123524",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Tiefe neuronale Netze haben sich zu sehr erfolgreichen und universellen Methoden in der Bildgebung und Bildrekonstruktion entwickelt. Sie liefern inzwischen für viele Anwendungen bessere Ergebnisse als klassische Methoden - von der Bildentrauschung bis hin zur Bildrekonstruktion aus wenigen verrauschten Messungen. Aus diesem Grund werden sie zunehmend in wichtigen Bildgebungstechnologien eingesetzt, beispielsweise in den neuesten Computertomographie-Scannern von GE. Es gibt vielfältige Möglichkeiten, neuronale Netze zur Lösung von inversen Problemen zu verwenden. Die beste Methode, aus einer Messung ein Bild zu rekonstruieren, hinsichtlich Rekonstruktionsgenauigkeit und -geschwindigkeit, ist in vielen Fällen ein trainiertes Neuronales Netz mit Faltungsstruktur oder ein Transformernetzwerk. Obwohl die Netzwerke empirisch sehr gut abschneiden, sind eine Reihe wichtiger theoretischer Fragen offen, die wir in diesem Projekt wie folgt angehen wollen: Generalisierung: In der ersten Phase dieses Projekts haben wir empirische und theoretische Fortschritte im Verständnis von Generalisierungsaspekten neuronaler Netzwerke in der Signalrekonstruktion für ein einfaches lineares Modell gemacht. In dieser Fortsetzung des Projekts ist es unser Ziel, realistischere Szenarien zu untersuchen, bei denen die Netzwerkstruktur und/oder die Menge realistischer Daten nichtlinear ist und ein neuronales Netzwerk mit Gradientenabstieg trainiert wird. Robustheit: Darüber hinaus haben wir in der ersten Phase des Projekts die Robustheit gegenüber feindlichem Rauschen für einen linearen Schätzer charakterisiert. Im beantragten Projekt werden wir deutlich über ein lineares Modell hinausgehen und Ergebnisse für einfache neuronale Netze beweisen. Quantifizierung von Unsicherheiten: Bei der Untersuchung der Robustheit haben wir erkannt, dass ein wichtiges Problem darin besteht, dass es bei der Verwendung neuronaler Netze nicht klar ist, wann sie wie akkurat sind, so dass realistische Bilder erzeugt werden können, die nicht der Realität entsprechen. Daher planen wir in der zweiten Phase dieses Projekts die Entwicklung neuer Methoden zur Quantifizierung der Unsicherheit neuronaler Netze für die Signalrekonstruktion.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "462234017",
      "url": "https://gepris.dfg.de/gepris/projekt/462234017",
      "title": "Meanfield Theorie zur Analysis von Deep Learning Methoden",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 462234017",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Kinetische Theorie hat sich in den letzten Jahren als eine mögliche geeignete mathematische Methode zur hierarchischen Beschreibung  einer Vielzahl physikalischer, technischer und soziologischer Prozesse bewährt. Zum Beispiel können strukturbildende Mechanism als Konsequenz interagierender Partikelsysteme mit Hilfe kinetischer Theorie sichtbar  und einer mathematischen Analyse zugänglich gemacht werden. Ausgangspunkt dieses Antrags ist die Beobachtung, dass einige moderne Learning Methods, zum Beispiel Deep Residual Neuronal Networks,  mathematisch ebenfalls als interagierende Partikelsysteme formuliert werden können. Hierbei bildet der Zustand des Partikels den  Aktivierungszustand eines Neurons ab. In diesem Antrag planen wir die existierenden Methoden der kinetischen Gastheorie und insbesondere des sog. Meanfield Grenzwertes zu nutzen, auf die Speizifika der Learning Methods zu adaptieren und zu erweitern, um schließlich eine mathematische Analyse dieser Methoden zu ermöglichen. Der Schwerpunkt der Untersuchung liegt hierbei auf den für einen Meanfield Grenzwert zugänglichen Deep Residual Neuronal Networks und Filtermethoden zum Erlernen von Modellparametern aus (verrauschten) Daten. Die u.U. zu erweiternden Methoden der kinetischen Gastheorie sollen benutzt werden, um an den geeignet und äquivalent reformulierten Dynamiken strukturbildende Mechanism erkennen und analysieren zu können. Neben der Erkenntnis über zugrundeliegende Wirkungsweisen der Lernverfahren ist auch das Ziel  damit neue, beweisbar konvergente und stabile Verfahren zu entwickeln. Die herzuleitenden partiellen Differentialgleichungen des Meanfield und/oder kinetischen Grenzwertes sollen hierbei erlauben, die dafür nötige Struktur und Formulierung zu liefern. Beispiele für geplante  Beiträge durch diesen Antrag sind schnelle und stabile Verfahren zum Trainieren (und Adaptieren) neuronaler Netze, Studien zur Robustheit im Hinblick auf zum Beispiel unsichere Daten, Fragen der Darstellbarkeit Neuronaler Netze und neue Verfahren für daten-basierte Parameterschätzungen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "463889763",
      "url": "https://gepris.dfg.de/gepris/projekt/463889763",
      "title": "Mehrphasige probabilistische Optimierer für tiefe Lernprobleme",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable ComputingMathematik",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463889763",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Dieser Projektvorschlag zu SPP 2298/1 schlägt die Entwicklung eines neuartigen Paradigmas für das Training tiefer neuronaler Netze vor. Die Besonderheiten tiefer Modelle, insbesondere starke Stochastizität (SNR<1), schließen die Verwendung klassischer Optimierungsalgorithmen aus. Und die Alternativen, von denen es inzwischen viele gibt, gehen verschwenderisch mit Ressourcen um. Anstatt der langen und wachsenden Liste solcher Methoden noch eine weitere Optimierungsregel hinzuzufügen, zielt dieses Projekt darauf ab, durch die Untersuchung von zwei Schlüsselideen wesentliche konzeptionelle Fortschritte zu erzielen: Erstens, die Nutzung der gesamten *Wahrscheinlichkeitsverteilung* von Gradienten über den Datensatz (bzw. einer empirischen Schätzung derselben), um algorithmische Parameter zu identifizieren, die der Benutzer sonst manuell einstellen müsste. Und zweitens die Aufspaltung des Optimierungsprozesses in mindestens drei verschiedene *Phasen* mit unterschiedlichen mathematischen Zielen. Das Ziel ist es, (konkret, als Software-Bibliothek) einen Optimierer zu entwickeln, der keine manuelle Abstimmung erfordert und eine gute Generalisierungsleistung ohne wiederholte Neustarts erreicht.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464103607",
      "url": "https://gepris.dfg.de/gepris/projekt/464103607",
      "title": "Multilevel-Architekturen und -Algorithmen im Deep Learning",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464103607",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Der Entwurf von tiefen neuronalen Netzen (DNNs) und deren Training ist ein zentrales Thema beim maschinellen Lernen. Fortschritte in diesen Bereichen sind eine der treibenden Kräfte für den Erfolg dieser Technologien. Trotzdem sind während des Lernprozesses häufig noch mühsame Experimente und menschliche Interaktion erforderlich, um eine geeignete Netzwerkstruktur und entsprechende Hyperparameter zu finden und so das gewünschte Verhalten eines DNNs zu erhalten.Das strategische Ziel des vorgeschlagenen Projekts besteht darin, algorithmische Techniken zur Verbesserung dieser Situation zu entwickeln. Unser methodischer Ansatz basiert auf gut etablierten mathematischen Strategien: Identifikation grundlegender algorithmischer Größen, Entwicklung geeigneter A-Posteriori-Schätzer, Identifikation eines geeigneten topologischen Rahmens für die gegebene Problemklasse und deren konsistente Nutzung sowie Einführung einer Multilevel-Struktur für DNNs basierend auf der Tatsache, dass DNNs lediglich eine diskrete Approximation einer kontinuierlichen nichtlinearen Abbildung darstellen, die Eingabe- auf Ausgabedaten abbildet. Durch Kombination dieser Idee mit neuartigen algorithmischen Steuerungsstrategien und Vorkonditionierern werden wir die neue Klasse adaptiver Multilevel-Algorithmen für Deep Learning etablieren, die nicht nur ein festes DNN optimieren, sondern die DNN-Architektur während der Optimierungsschleife adaptiv verfeinern und erweitern. Dieses Konzept ist nicht auf eine bestimmte Netzwerkarchitektur beschränkt, und wir werden vorwärtsgerichtete neuronale Netze, ResNets und PINNs als relevante Beispiele untersuchen.Unser integrierter Ansatz wird daher viele der derzeitigen manuellen Tuning-Techniken durch auf A-Posteriori-Schätzern basierende algorithmische Strategien ersetzen können. Darüber hinaus wird unser Algorithmus den Rechenaufwand für das Training und auch die Größe des resultierenden DNN im Vergleich zu einem manuell entworfenen Gegenstück reduzieren, wodurch die Verwendung von Deep Learning in vielerlei Hinsicht effizienter wird. Schließlich hat unser algorithmischer Ansatz auf lange Sicht das Potenzial, die Zuverlässigkeit und Interpretierbarkeit des resultierenden trainierten DNN zu verbessern.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "463883089",
      "url": "https://gepris.dfg.de/gepris/projekt/463883089",
      "title": "Multiskalendynamik neuronaler Netze über stochastische Graphoperatoren",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2026",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463883089",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Deep Learning in künstlichen neuralen Netzwerken ist ein äußerst effektives und flexibles Instrument zur Lösung hochkomplizierter analytischer und prädiktiver Aufgaben in verschiedensten Anwendungen. Ein vollständiges theoretisches, insbesondere mathematisches, Verständnis von Deep Neural Networks und der damit verbundenen Algorithmen ist bisher nicht möglich. Das vorliegende Forschungsvorhaben zielt darauf ab, mehrere aktuelle Methoden aus der Theorie der dynamischen Systeme dazu zu nutzen, ein neues rigoroses Gerüst für maschinelles Lernen im Kontext von Deep Neural Networks zu entwickeln.Ein Kernpunkt unserer Analyse solcher Netzwerke besteht darin, zwei verschiedene dynamische Zeitskalen auszumachen, die dann in einem stochastischen Multiskalensystem zusammengebracht werden können. Auf der einen Seite widmen wir uns der \"schnellen\" Dynamik, die in Form der Informationsausbreitung durch das große adaptive Netzwerk gegeben ist. In diesem Zusammenhang sollen Grenzsysteme von integro-partiellen Differentialgleichungen im unendlichen Netzwerklimes hergeleitet und analysiert werden, indem die Theorie von Graphoperatoren, auch graphops genannt, genutzt und weiterentwickelt wird. Auf der anderen Seite soll die \"langsame\" stochastische Dynamik des Lernvorgangs untersucht werden, die in der Anpassung der Gewichte im adaptiven Netzwerk besteht; hierbei setzen wir den Fokus auf stochastischen Gradientenabstieg, seine Metastabilitätseigenschaften und die Interpretation über zufällige dynamische Systeme.Die zwei dynamischen Skalen werden in einem dritten Schritt zu einem vollständigen Model des neuronalen Netzes zusammengeführt, sodass der Zusammenhang von stochastischem Lernen, dynamischer Robustheit und analytischer Expressivität erfasst werden kann; dabei erwarten wir, bestimmte Muster und ihre repräsentative Bedeutung erklären und vorhersagen zu können. Die mathematische Beschreibung stützt sich dabei auf verschiedene Bereiche wie stochastische Dynamik, Ergodentheorie, adaptive Netzwerke, Graphlimites und Multiskalendynamik.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "463912816",
      "url": "https://gepris.dfg.de/gepris/projekt/463912816",
      "title": "Nichtlineare optimale Feedback-Regelung mit tiefen neuronalen Netzen ohne den Fluch der Dimension: Räumlich abnehmende Sensitivität und nichtglatte Probleme",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463912816",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Optimale Regelung mittels Feedback ist eines der Hauptanwendungsgebiete von Deep Learning. Deep Reinforcement Learning, eines der Verfahren zur Berechnung optimaler Feedbacks und wohl einer der erfolgreichsten Algorithmen der Künstlichen Intelligenz, steht hinter den spektakulären Erfolgen in Spielen wie Schach und Go, hat aber auch mannigfache Anwendungen in Wissenschaft, Technik und Wirtschaft. Dieses Projekt erforscht die mathematischen Grundlagen dieser Erfolge. Wir konzentrieren uns dabei auf die Herleitung von Bedingungen, unter denen die hochdimensionalen Funktionen, die in der optimalen Regelung benötigt werden, effizient (also ohne den Fluch der Dimension) durch tiefe neuronale Netze (DNNs) approximiert werden können. Genauer betrachten wir einerseits optimale Wertefunktionen, die als eindeutige Viskositätslösungen von Hamilton-Jacobi-Bellman PDEs charakterisiert sind. Andererseits betrachten wir Kontroll-Lyapunovfunktionen (clfs). Diese treten an die Stelle der optimalen Wertefunktionen in Kontrollproblemen, in denen eine gegebene Menge oder ein Punkt asymptotisch - aber nicht unbedingt optimal - stabilisiert werden soll. Beide Funktionsarten liefern die Grundlage zur Berechnung von optimalen oder stabilisierenden Feedback-Kontrollen, die das ultimative Ziel bei der Lösung von Kontrollproblemen auf langem oder unendlichen Zeithorizont sind. In der ersten Förderperiode haben wir verschiedene Bedingungen an die Problemdaten, also an die Dynamik und die Kostenfunktion identifiziert, unter denen sich die genannten Funktionen durch kompositionelle oder separable Funktionen approximieren lassen, die auch in hohen Dimenionen effizient durch DNNs darstellbar sind. Die wahrscheinlich wichtigste Erkenntnis aus der ersten Förderperiode ist, dass räumlich abnehmende Sensitivitäten der Schlüssel zur Konstruktion einer überlappenden separablen Approximation einer optimalen Wertefunktion sind. Diese abnehmenden Sensitivitäten wurden in jüngster Zeit vielfach untersucht (in zeitlicher Form unter anderem auch vom Antragsteller) und das Verständnis ihrer Bedeutung für DNN Approximationen wird einer der zentralen Punkte in der zweiten Förderperiode sein. Eine Beschränkung der Ergebnisse aus der ersten Förderperiode besteht darin, dass sie derzeit nur für glatte optimale Wertefunktionen bzw. clfs und nur für DNNs mit glatten Aktivierungsfunktionen gelten. Letzteres schließt die beliebten und effizient implementierbaren ReLU DNNs aus, ersteres alle Kontrollprobleme ohne glatte Lösung, wie z.B. asymptotische Stabilisierungsprobleme mit Hindernissen. Für diese Problemklasse ist bekannt, dass nur aus nichtglatten Näherungen Feedback-Kontrollen berechnet werden können, die das Kontrollproblem tatsächlich lösen. Der zweite zentrale Punkt der zweiten Förderperiode wird daher die Entwicklung von approximativen ReLU DNNs für Probleme mit nichtglatten Lösungen sein, die auf den Resultaten für glatte Probleme aus der ersten Förderperiode aufbauen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "543965776",
      "url": "https://gepris.dfg.de/gepris/projekt/543965776",
      "title": "Operator Learning für Optimalsteuerung: Approximation und Statistische Theorie",
      "investigators": "Dr. Evelyn  Herberg;Professor Dr. Markus  Reiß, seit 11/2025;Professor Dr. Jakob  Zech",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 543965776",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Dieser Antrag konzentriert sich auf die Entwicklung einer umfassenden Konvergenzanalyse für 'Operator-Learning', eine aufkommende Methode, die für die effiziente Annäherung von Daten-zu-Lösungs-Abbildungen bei parameterabhängigen partiellen Differentialgleichungen (PDEs) eingesetzt wird. Als Anwendung betrachten wir die Lösung von Optimalsteuerungsproblemen, die typischerweise auf sequentielle, kostspielige numerische PDE-Lösungen angewiesen sind. Zu diesem Zweck führen wir eine vollständige Fehleranalyse durch, die darauf abzielt, den 'Bias-Variance-Tradeoff' zu analysieren und den sogenannten 'Fluch der Dimensionalität' durch die Auswahl geeigneter Netzwerkarchitekturen zu vermeiden. Darüber hinaus werden die gewonnenen Erkenntnisse auch für Problemstellungen im Bereich der Unsicherheitsquantifizierung und der Parameterschätzung Anwendung finden. Das Projekt wird sich mit der Entwicklung geeigneter Netzwerkarchitekturen befassen, wobei der Schwerpunkt auf deren Expressivität und Fehleranalyse liegt. Aufbauend auf diesen Ergebnissen werden wir zudem eine statistische Analyse für den empirischen Risikominimierer durchführen. Dieses Projekt wird somit klassische Resultate aus der Regressionsanalyse auf nichtlineare Abbildungen zwischen unendlich dimensionalen Räumen erweitern. Wichtige Hilfsmittel zur Erreichung dieser Ziele sind die Nutzung niedrigdimensionaler Strukturen, die sich aus der bekannten hohen Regularität von Parameter-zu-Lösungs-Abbildungen ergeben, sowie die Auftrennung der In- und Outputs des Operators in wichtige niederfrequente und weniger wichtige hochfrequente Teile. Dies wird durch eine so genannte Encoder/Decoder-Architektur erreicht, die es ermöglicht, In- und Outputs in stabilen Darstellungssystemen wie Wavelets darzustellen. Der praktische Aspekt des Projekts besteht in der Integration von Operator Learning Modellen in Optimierungsalgorithmen, um den Rechenaufwand für die Lösung von PDE-beschränkten Optimalsteuerungsproblemen erheblich zu verringern.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep LearningEhemaliger AntragstellerProfessor Dr. Sven  Wang, bis 11/2025"
    },
    {
      "project_id": "543963649",
      "url": "https://gepris.dfg.de/gepris/projekt/543963649",
      "title": "Optimaler Transport und Maßoptimierungsgrundlage für robustes und kausales maschinelles Lernen",
      "subject_area": "MathematikTheoretische Informatik",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 543963649",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Der Projektvorschlag konzentriert sich auf die mathematische Grundlage zum Verständnis und zur Bewältigung von Verteilungsverschiebungen in der maschinellen Lernens (ML), insbesondere für den Umgang mit strukturierten Verteilungsverschiebungen und kausalen Schlussfolgerungen mit hochdimensionalen Daten. Dies beinhaltet den Umgang mit Diskrepanzen zwischen Trainings- und Testdatenverteilungen. Das Projekt zielt darauf ab, ein rigoroses theoretisches Rahmenwerk zum Verständnis und zur Bewältigung strukturierter Verteilungsverschiebungen mithilfe von variationalen Ansätzen zu dynamischen Systemen, Gradientenflüssen von Wahrscheinlichkeitsmaßen und optimalem Transport zu entwickeln. Die Hauptziele umfassen: - Die Entwicklung variationaler Modelle für strukturierte Verteilungsverschiebungen in hochdimensionalen Daten. Dies beinhaltet die Verwendung von Gradientenflusstheorie für die Maßoptimierung in neuen Geometrien, die für strukturierte Verteilungsverschiebungen geeignet sind. - Die Etablierung eines einheitlichen Rahmens zur Maßoptimierung für die Modellierung und Bewertung von Verteilungsverschiebungen. Dieser Rahmen wird das Wissen im Bereich der Maßoptimierung vorantreiben und ein robusteres und vertrauenswürdigeres Modell für ML-Training, Datengenerierung und Vorhersage bieten. Anschließend werden wir strukturierte verteilungsrobuste Lernalgorithmen untersuchen. - Die Erforschung modernster flussbasierter generativer Modelle zur Modellierung strukturierter Verteilungsverschiebung.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "543939932",
      "url": "https://gepris.dfg.de/gepris/projekt/543939932",
      "title": "Robuste und Interpretierbare Regularisierung für Inverse Probleme in der Medizinischen Bildverarbeitung",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 543939932",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Viele Aufgaben in den bildgebenden Wissenschaften lassen sich als inverse Probleme modellieren. Dabei muss ein unbekanntes Bild, zum Beispiel das Innere des menschlichen Körpers, aus verrauschten Messdaten rekonstruiert werden. Bekannte Beispiele aus der Medizin sind die Röntgen-Computertomographie (CT) und die Magnetresonanztomographie (MRT), welche im Klinikalltag unverzichtbar sind. Bei vielen inversen Problemen ist die Rekonstruktion aufgrund starken Rauschens oder mangelnden Daten schwierig. Daher besteht stets ein Bedarf an noch besseren Lösungsmethoden. Bis zuletzt waren theoretisch motivierte variationelle Methoden das Maß der Dinge. Bei diesen löst die Rekonstruktion ein Optimierungsproblems mit einem Regularisierer, welcher meist basierend auf Vorkenntnissen über die erwünschten Lösungen von Experten entworfen wird. Durch die explizite händische Modellierung sind solche Methoden interpretierbar. Zuletzt wurden Deep-Learning (DL) Methoden in vielen Bereichen immer populärer, da diese wesentlich bessere Rekonstruktionen liefern. Allerdings fehlt ihnen oft eine klare theoretische Begründung und daher auch die Interpretierbarkeit. Folglich gibt es eine anhaltende Debatte bezüglich der klinischen Anwendbarkeit, da Fehldiagnosen hier fatal sein können. In diesem Projekt wollen wir das Beste aus beiden Welten kombinieren und neuartige lernbare Regularisierer entwickeln. Dabei sollen Interpretierbarkeit und theoretische Garantien erhalten bleiben. Beim Vergleich mit erfolgreichen DL Modellen haben wir zwei wesentliche Konzepte identifiziert, die in händisch modellierten Regularisierern kaum präsent sind: lokale Adaptivität und globale Abhängigkeiten. Adaptivität bezieht sich dabei auf die Fähigkeit, lokale Strukturen in der Regularisierungsstärke zu berücksichtigen, und somit scharfe oder akzentuierte Strukturen in der Rekonstruktion besser zu erhalten. Darüber hinaus modellieren globale Abhängigkeiten Eigenschaften des gesamten Bildes, wie zum Beispiel Symmetrien, Muster oder charakteristische Objekte. Diese erfordern einen Informationsaustausch über weite Strecken, welcher mit der lokalen Natur der meisten händisch modellierten Regularisierer konkurriert. Ausgehend von lokalen Regularisierern wollen wir beide Prinzipien durch Konditionierung und Multiskalenmodellierung einbeziehen. Erste numerische Experimente mit konditionierten Regularisierern haben tatsächlich zu deutlich verbesserten Rekonstruktionen geführt. Weiterhin können wir durch sorgfältiges Design der Regularisierer die Interpretierbarkeit weitgehend aufrechterhalten und auch theoretische Garantien erhalten. Im Vergleich zu klassischen Ansätzen führt die Konditionierung zu einer Abhängigkeit des Regularisierers von den Messdaten. Daher wird eine völlig neue theoretische Analyse notwendig. Schließlich werden wir die gewonnenen theoretischen Erkenntnisse an anspruchsvollen realen inversen Problemen, wie zum Beispiel der kürzlich populär gewordenen Niedrig-Feld MRT, verifizieren.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep LearningInternationaler BezugÖsterreich"
    },
    {
      "project_id": "543965165",
      "url": "https://gepris.dfg.de/gepris/projekt/543965165",
      "title": "Spiking Neural Networks: Theoretische Grundlagen, Vertrauenswürdigkeit und Energie-Effizienz",
      "investigators": "Professor Dr.-Ing. Holger  Boche;Professorin Dr. Gitta  Kutyniok",
      "subject_area": "MathematikKommunikationstechnik und -netze, Hochfrequenztechnik und photonische Systeme, Signalverarbeitung und maschinelles Lernen für die Informationstechnik",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 543965165",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Obwohl Deep Learning bemerkenswerte Erfolge vorweisen kann, ist es trotzdem essentiell, seine Grenzen und Nachteile zu erkennen und anzugehen. Gegenwärtig weisen Deep-Learning-Methoden inhärente Probleme auf in Bezug auf Vertrauenswürdigkeit, wobei der Begriff \"Vertrauenswürdigkeit\" Aspekte wie Datenschutz, Sicherheit, Belastbarkeit, Zuverlässigkeit und Verantwortlichkeit umfasst.  Kürzlich wurde gezeigt, dass der Erfolg von algorithmischen Berechnungen beim Deep Learning sogar von der verwendeten Computerplattform abhängen kann. Insbesondere die allgegenwärtige digitale Hardware, auf der Deep-Learning-Modelle in der Regel implementiert werden, weist Einschränkungen auf, die ein vertrauenswürdiges Deep Learning unter bestimmten Umständen verhindern können. Neben der Vertrauenswürdigkeit ist ein weiteres Problem im Zusammenhang mit Deep Learning der rasch steigende Bedarf an Rechenleistung. Da die Modelle immer komplexer werden und mehr Rechenressourcen benötigen, setzt sich zunehmend die Erkenntnis durch, dass die derzeitigen Technologien aufgrund ihres Energiebedarfs langfristig nicht tragbar sind. Daraus ergibt sich die zentrale Frage: Können wir diese Herausforderungen überwinden, um eine vertrauenswürdige und energieeffiziente künstliche Intelligenz zu schaffen? Eine mögliche Lösung, ohne die Fähigkeiten des Deep Learning zu beeinträchtigen, könnte der Einsatz von Spiking Neural Networks auf neuromorpher Hardware sein. Dies würde einen Wechsel auf der Softwareseite - von klassischen neuronalen Netzen zu weiterentwickelten Spiking Neural Networks - und auf der Hardwareseite - von einem klassischen digitalen Computermodell zu einem neuromorphen Ansatz, der für die Implementierung von Spiking Neural Networks optimiert ist - bedeuten. Das Ziel dieses Antrags ist die theoretische Analyse von Spiking Neural Networks in Kombination mit neuromorpher Hardware im Rahmen von vertrauenswürdigen und energieeffizienten Berechnungen. Unsere Hauptziele sind (1) die Untersuchung der Approximationseigenschaften von Spiking Neural Networks, insbesondere durch das Spike Response und Leaky-Integrate and Fire Model, (2) die Analyse theoretischer Aspekte der Algorithmen zum effektiven Training von Spiking Neural Networks, der Beweis von Fehlerschranken für die Generalisierung und die Durchführung umfassender numerischer Experimente zur Validierung der Ergebnisse, (3) die Einführung von Metriken zur Messung der Energieeffizienz und Durchführung einer vergleichenden Analyse der Implementierung von Spiking Neural Networks auf digitaler und neuromorpher Hardware und (4) die Untersuchung verschiedener Computermodelle und entsprechender Hardwareplattformen für die Implementierung von vertrauenswürdigen Spiking Neural Networks.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "463402401",
      "url": "https://gepris.dfg.de/gepris/projekt/463402401",
      "title": "Statistische Grundlagen des halb-überwachten Lernens mit Graph-Neural-Networks",
      "subject_area": "Theoretische InformatikMathematik",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463402401",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Die Theorie des Deep Learnings ist ein aktiver Forschungsbereich und hat ein genaues Verständnis der Leistung überwachter Modelle ermöglicht, die auf markierten Daten trainiert werden. Doch die praktischen Entwicklungen im Bereich von Foundation-Models hängen in hohem Maße von der Verfügbarkeit riesiger Mengen unmarkierter Daten ab. Daher ist es ebenso wichtig zu verstehen, wie in der Praxis durch halb- oder unüberwachtes Deep Learning aus unmarkierten Daten bessere Modelle gelernt werden. Dies war das Ziel unseres Projekts in der ersten Förderphase des Schwerpunktprogramms und hat zu Ergebnissen sowohl zum unüberwachten Repräsentationslernen als auch zum halbüberwachten Deep Learning auf Graphen geführt. Das Ziel des Nachfolgeprojekts besteht darin, zwei wichtige Fragen des modernen maschinellen Lernens zu verstehen: (i) Wie verbessern unmarkierte Daten die Leistung von Modellen? (ii) Warum verbessern der Attention-Mechanismus und die Transformer-Architektur die statistische Leistung? Wir beantworten beide Fragen im Kontext des halbüberwachten Deep Learnings auf Graphen, insbesondere durch die Untersuchung von Graph-Neural-Networks für die Klassifizierung von Knoten und die Vorhersage von Kanten. Die wichtigsten technischen Beiträge des Projekts sind: (i) Herleitung des Neural-Tangent-Kernels für unendlich breite neuronale Netze, und des Gaußprozess-Grenzwertes für Graph-Attention-Networks und für Graph-Transformer. (ii) Berechnung des exakten statistischen Risikos für Kernel-basierte Approximationen von Graph-Neural-Networks, einschließlich Faltungs- und Attention-basierter Architekturen; (iii) Statistische Garantien für Graph-Neural-Networks für kontextbezogene stochastische Blockmodelle. Die Ergebnisse werden es ermöglichen, die statistische Leistung von Faltungs- und Attention-basierter Modellen genau zu vergleichen und so die Frage zu beantworten, warum tiefe Attention-basierte Modelle für das Lernen von Daten mit Interaktionen großer Reichweite überlegen sind. Darüber hinaus wird die Analyse von Zufallsgraphen Aufschluss darüber geben, wann zusätzliche Informationen (hier gegeben in Form von Graphen) die Vorhersagekraft von Modellen verbessern oder verschlechtern können. Die Analyse wird auch dazu beitragen, die Grenzen aktueller Graph-Neural-Networks zu identifizieren und alternative Architekturen zu erforschen, die für kontextbezogene stochastische Blockmodelle nahezu optimal sind.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464075789",
      "url": "https://gepris.dfg.de/gepris/projekt/464075789",
      "title": "Strukturerhaltende tiefe neuronale Netze zur Beschleunigung der Lösung der Boltzmanngleichung",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464075789",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Unser Ziel ist es zu zeigen, dass der gemeinsame Einsatz von tiefen neuronalen Netzen und klassischen numerischen Methoden möglich und vorteilhaft für die Lösung von PDEs ist. Wir werden dies am Beispiel der Boltzmann-Gleichung demonstrieren, die ein anspruchsvolles Anwendungsproblem darstellt, das einerseits eine komplizierte analytische Struktur aufweist, die mit numerischen Methoden erhalten werden sollte, und dessen Lösung andererseits selbst auf den größten heutigen Supercomputern noch unerreichbar ist. In diesem zweiten Teil des Projekts konzentrieren wir uns im Detail auf die Ersetzung des Kollisionsoperators durch ein tiefes neuronales Netz. Wir verwenden ein DeepONet zur Approximation des nichtlinearen Integraloperators. Die Herausforderungen bestehen darin, die Erhaltungseigenschaften und die Entropiedissipation numerisch sicherzustellen. Die Erhaltung wird durch die Erweiterung der Basis im DeepONet erreicht, die im Trunk Net kodiert ist. Die Entropiedissipation wird durch eine neue Interpretation der Boltzmann-Gleichung als Gradientenfluss und durch die Annäherung des symmetrischen positiv-definiten metrischen Tensors, der mit dem Gradientenfluss verbunden ist, sichergestellt. Wir validieren unsere Methoden in realistischen Testfällen und stellen den gesamten Quellcode zur Verfügung.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464101190",
      "url": "https://gepris.dfg.de/gepris/projekt/464101190",
      "title": "Theoretischer Grundlagen des Unsicherheits-robusten Deep Learning für Inverse Probleme",
      "subject_area": "MathematikBild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464101190",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Dieses Projekt setzt die Entwicklung theoretisch fundierter Deep Learning Methoden für inverse Probleme mit einem besonderen Augenmerk auf mögliche Unsicherheiten fort. Ein zentrales Problem in Machine Learning Methoden für inverse Probleme ist der mögliche Unterschied zwischen Trainingdaten und jene, die später wirklich in der Anwendung auftreten, etwa da man auf simulierten Daten trainiert. Wir werden dafür eine passende mathematische Theorie entwickeln um diese Unsicherheiten in den Trainingsdaten zu behandeln und die Methoden robust dagegen zu machen. Darüber hinaus werden wir neue Methoden entwickeln um Deep Learning auch für den Fall, dass nicht genügend Trainingsdaten für überwachtes Lernen vorliegen, anwendbar zu machen. Ein zweites Ziel des Projekts ist es Methoden zur Quantifizierung von Unsicherheiten in Deep Learning Ansätzen für Inverse Probleme zu entwickeln. Dies werden wir im Rahmen Bayesianischer Modellierung durch geeignetes Sampling erreichen, wofür geeignete Techniken basierend auf Diffusion entwickeln.  Weiters wollen wir neue Ansätze entwickeln um die Diversität möglicher Lösungen, die gemessene Daten erklären könnten, zu berechnen. Der letzte ist Schritt ist es Unsicherheits-robustes Deep Learning hin zu großen Anwendungen zu entwickeln. Dies werden wir an Hand aktueller Techniken in der Bildgebung mit Röntgenstrahlen und optischem Licht durchführen, inklusive des Problems der Phasenrekonstruktion.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464252197",
      "url": "https://gepris.dfg.de/gepris/projekt/464252197",
      "title": "Theorie der tiefen Anomalieerkennung",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464252197",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Anomalieerkennung ist ein fundamentales Problem des Machinellen Lernens mit Anwendungen in der Fehlererkennung und Qualitätskontrolle in Produktionsprozessen, in der Sicherung digitaler Systeme und in Frühwarnsystemen. Durch den Einsatz tiefer Neuronaler Netze sind viele komplexe Benchmarks praktisch effektiv gelöst. Dennoch gibt es kaum Theorie diese Ergebnisse zu erklären und die Methoden zu analysieren. Dies gilt insbesondere für die Anomalieerkennung, deren Theorie sich größtenteils auf nicht-tiefe Methoden beschränkt. Das Ziel dieses Projekts ist die Entwicklung einer Theorie für tiefe Anomalieerkennung. Eine solche Theorie wird es ermöglichen, bestehende Methoden zu verstehen und neue Methoden zu entwickeln. Aufgrund fehlender expliziter Charakterisierung der Verteilung, der die anomalen Daten unterliegen, lassen sich existierende Resultate nicht einfach auf Anomalieerkennung anwenden. Es müssen neue Wege beschritten werden: Es müssen neue Werkzeuge entwickelt werden, und es müssen neue Herangehensweisen erdacht werden. In diesem Projekt beschreiben wir unser Vorhaben, eine Theorie für die tiefe Anomalieerkennung zu entwickeln. Das Programm umfasst Schranken für die Performanz und Garantien für die Konvergenz mit endlich und unendlich vielen Daten. Hierzu beziehen wir den Optimierungsalgorithmus explizit in die Analyse ein, um letztendlich praktisch relevante Schranken zu finden. Aktuelle Methoden weichen bewusst von der bisher fundamentalen Annahme ab, dass Anomalien uniform verteilt sind, und erzielen konsistent herausragende Ergebnisse. Basierend auf unserer Konsistenzanalyse werden wir diese Annahme und deren Folgen reevaluieren. Dies wird uns erlauben auch moderne Methoden in unsere Theorie zu integrieren.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464101154",
      "url": "https://gepris.dfg.de/gepris/projekt/464101154",
      "title": "Tiefe neuronale Netzwerke überwinden den Fluch der Dimensionalität in der numerischen Approximation von stochastischen Kontrollproblemen und von semilinearen Poisson Gleichungen",
      "subject_area": "Mathematik",
      "funding_period": "Förderung von 2021 bis 2026",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464101154",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Partielle Differentialgleichungen (PDGen) sind ein Schlüsselinstrument bei der Modellierung vieler naturwissenschaftlicher und ökonomischer Phänomene. Viele der PDGen, die in der Finanztechnik, Wirtschaftstheorie, Quantenmechanik oder statistischen Physik auftreten, sind nichtlinear, hochdimensional und können nicht explizit gelöst werden. Die Entwicklung numerischer Verfahren, welche nachweisbar die Lösungen dieser hochdimensionalen, nichtlinearen PDGen approximieren und nicht unter dem sogenannten Fluch der Dimensionalität leiden, ist eine höchst anspruchsvolle Herausforderung. Neuronale Netzwerke und andere auf tiefem Lernen basierende Methoden wurden in letzter Zeit sehr erfolgreich auf eine Reihe numerischer Probleme angewandt. Insbesondere weisen Simulationen darauf hin, dass Algorithmen, die auf neuronalen Netzwerken basieren, den Fluch der Dimensionalität bei der numerischen Approximation von Lösungen bestimmter nichtlinearer PDGen überwinden. Für bestimmte lineare und nichtlineare PDGen ist dies auch mathematisch nachgewiesen worden.  Das Hauptziel dieses Projektes ist es, zum ersten Mal rigoros zu beweisen, dass neuronale Netzwerke den Fluch der Dimensionalität überwinden bei der numerischen Lösung nichtlinearer PDGen, die bei stochastischen Kontrollproblemen entstehen, und bei der numerischen Lösung semilinearer Poisson-Gleichungen mit Dirichlet-Randbedingungen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "463952752",
      "url": "https://gepris.dfg.de/gepris/projekt/463952752",
      "title": "Tiefe Zuweisungsflüsse für die strukturierte Klassifikation von Daten: Entwurf, Lernverfahren und prädiktive Genauigkeit",
      "subject_area": "MathematikBild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2021",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 463952752",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Dieses Projekt fokussiert auf eine Klasse zeitkontinuierlicher neuronaler gewöhnlicher Differentialgleichungen für die Kennzeichnung metrischer Daten auf Graphen, um aus drei Stoßrichtungen zur Theorie des \"tiefen Lernens\" beizutragen: (i) Anwendung der Informationsgeometrie für den Entwurf und das Verständnis tiefer Netze im Zusammenhang mit strukturierten Vorhersagen und maschinellem Lernen; (ii) Geometrische Charakterisierung der Dynamik des Lernens von Parametern und ihrer Interaktion mit sich entwickelnden Netzwerkzuständen, als Modell kontextsensitiver Entscheidungen; (iii) Studium von PAC-Bayesschen Risikoschranken zur Quantifizierung der statistischen Vorhersagegenauigkeit von Klassifikationen und strukturierter Kennzeichnungen mittels tiefer Zuweisungsflüsse.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "543963829",
      "url": "https://gepris.dfg.de/gepris/projekt/543963829",
      "title": "Unsicherheitsquantifizierungsgarantien für Graphenneuronale Netze",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 543963829",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "In einer vernetzten Welt sind Graphen überall zu finden. Fast alle Graph-Lernaufgaben werden routinemäßig und effektiv mit Graphenneuronale Netze (GNNs) angegangen, die sich als grundlegender Baustein in vielen Systemen etabliert haben. Trotz des Aufstiegs der GNNs zu Bedeutung in wissenschaftlichen und industriellen Umgebungen gibt es auffällig wenig Forschung zur Quantifizierung ihrer Unsicherheit, und noch weniger Studien zielen darauf ab, Garantien zu bieten. Um diese Lücke zu schließen, ist unser übergeordnetes Ziel, eine Unsicherheitsquantifizierung für GNNs mit rigorosen verteilungsfreien Garantien zu bieten. Wir werden den Rahmen der konformen Vorhersage übernehmen, der die Ausgabe jedes Modells (jedes GNN) in eine Menge umwandelt, die garantiert das wahre Label mit einer vom Benutzer angegebenen Wahrscheinlichkeit enthält. Wir werden austauschbare Daten unter transduktiven und induktiven Einstellungen, Robustheit gegenüber Verteilungsverschiebungen und adversativen Störungen sowie Garantien für beliebige (potenziell sich überschneidende) Subpopulationen abdecken. Angesichts der vielen verschiedenen Anwendungen von GNNs erwarten wir, dass unsere Ergebnisse für Wissenschaft, Industrie und Gesellschaft breit relevant sind.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "543965508",
      "url": "https://gepris.dfg.de/gepris/projekt/543965508",
      "title": "Verbesserte Techniken des tiefen Lernens zum Sampling von Moleküldynamik im thermodynamischen Gleichgewicht mithilfe von optimalem Transport",
      "subject_area": "Mathematik",
      "funding_period": "Förderung seit 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 543965508",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Molekulardynamiksimulationen spielen eine wichtige Rolle in der Physik, der Chemie und der Biochemie, leiden jedoch unter dem Fluch der Dimension. In einer Molekulardynamiksimulation mit N Atomen oder Subsystemen ist die Boltzmann-Verteilung, die das Verhalten im thermodynamischen Gleichgewicht bestimmt, ein Wahrscheinlichkeitsmaß auf dem hochdimensionalen Raum R^3N. Aktuelle Arbeiten, die in einer wegweisenden Arbeit von Noe et al. initiiert wurden, haben die Effektivität von Deep-Learning-Methoden bei der Probenahme der Boltzmann-Verteilung für große Systeme gezeigt. Genauer gesagt wurde gezeigt, dass Normalizing Flows, eine Art neuronales Netz, das den Transport von Maßen approximiert, effektiv Proben aus der Boltzmann-Verteilung entnimmt, indem es Proben aus einem einfacheren Wahrscheinlichkeitsmaß, wie zum Beispiel einer Gaußschen Verteilung, in Proben aus der gewünschten Verteilung umwandelt. Diese Techniken wurden jedoch bisher nicht mathematisch für die tatsächlichen Systeme von physikalischem Interesse gerechtfertigt. Unser Hauptziel in diesem Projekt ist es, die Techniken der Normalizing Flows auf ein solides mathematisches Fundament zu stellen, indem wir Methoden aus dem optimalen Transport verwenden, die unseres Wissens nach bisher in diesem Zusammenhang keine Rolle gespielt haben. Eine Hauptidee besteht darin, Normalizing Flows nicht nur darauf zu trainieren, einen Diffeomorphismus zu approximieren, der ein einfaches Referenzmaß auf die Boltzmann-Verteilung abbildet, sondern ihn darauf zu trainieren, den spezifischen Diffeomorphismus des optimalen Transports zu approximieren. Letzterer ist eindeutig und weist eine höhere Regularität auf.Dank aktueller Fortschritte im maschinellen Lernen kann diese Abbildung aufgrund ihrer höheren Regularität von tiefen neuronalen Netzen mit einer moderaten Anzahl von Parametern und Schichten approximiert werden. Darüber hinaus wird erwartet, dass die Eindeutigkeit der Zielabbildung und ihre Glattheit ein effizienteres Training und stabilere Stichprobenentnahme ermöglichen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep Learning"
    },
    {
      "project_id": "464104047",
      "url": "https://gepris.dfg.de/gepris/projekt/464104047",
      "title": "Über die Konvergenz von Variational Deep Learning zu Entropie-Summen",
      "investigators": "Professorin Dr. Asja  Fischer;Professor Dr. Jörg  Lücke",
      "subject_area": "MathematikBild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2021 bis 2026",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 464104047",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2298: \nTheoretische Grundlagen von Deep Learning",
      "description": "Tiefe probabilistische Modelle sind ein zentrales theoretisches Fundament des tiefen unüberwachten Lernens. In der Form z.B. von Sigmoid-Belief-Networks (SBNs) oder Deep-Belief-Networks (DBNs) haben sie eine Schlüsselrolle in der Ethablierung des gesamten Deep Learning Feldes gespielt. Heute sind tiefe probabilistische Modelle eine zentrale, treibende Kraft von theoretischen und praktischen Entwicklungen im Deep Learning z.B. in der Form von Variational Autoencoders (VAEs),  Generative Adversarial Nets oder Deep Restricted Boltzmann Machines (RBMs).Für alle tiefen probabilistischen Modelle ändert das Lernen die Modellparameter bis diese konvergiert sind, d.h., bis sich die Modellparameter nicht mehr wesentlich ändern. Die Konvergenzpunkte des Lernens können daher als stationäre Punkte einer Lerndynamik angesehen werden. Die konkreten Änderungen der Modellparameter werden durch die entsprechende Zielfunktion eines tiefen Modells definiert. Für die meisten probabilistischen Modelle (einschl. SBNs, RBMs und aller VAE-Varianten) ist diese Zielfunktion theoretisch wohl-definiert fundiert und entspricht der sog. Variational-Lower-Bound. Unser Ziel ist es hier, eine theoretische Struktur zu untersuchen, die allen tiefen Modellen, die auf der Variational-Lower-Bound beruhen, gemeinsam zu sein scheint: wir möchten untersuchen ob und wie die Lower-Bound zu Werten konvergiert, die durch eine Summe von Entropien beschrieben wird.Jedes tiefe probabilistische Modell wird durch individuelle Verteilungsfunktionen definiert, z.B. für die latenten und die observierten Variablen im Falle von SBNs und DBNs oder durch spezifische Boltzmann Verteilungen für RBMs. Unsere Hypothese ist es, dass während des Lernens die Parameter aller oben erwähnter Modelle sich so ändern, dass die Werte der Lower-Bound identisch werden zu einer Summe von Entropien. Diese Entropien sind genau die Entropien der Verteilungsfunktionen, durch die ein gegebenes tiefes Modell beschrieben wird. Unser Ziel ist es außerdem zu untersuchen, für welche allgemeine Klasse von tiefen Modelle dieses Konvergenzverhalten unter realistischen Bedingungen bewiesen werden kann.Darüber hinaus ist es unser Ziel die Konvergenz zu Entropie-Summen zur Verbesserung des tiefen unüberwachten Lernens zu benutzen. Dies soll erreicht werden (A) durch die Nutzung unser theoretisch Ergebnisse um flache lokale Optima, Mode-Collapse und Überanpassung zu vermeiden; und (B) durch die partielle analytische Lösung des Optimierungsproblems für tiefes Lernen. Unsere wichtigsten mathematischen Werkzeuge werden hierbei die Theorie der exponentiellen Familie von Wahrscheinlichkeitsverteilungen so wie die Theorie des Variational Deep Learning sein. Wir glauben, dass die Ergebnisse über die hier untersuchte theoretischen Struktur zu einem neuen und tieferen Verständnis des tiefen unüberwachten Lernens führen wird; und wir glauben, dass dieses tiefere Verständis zu wesentliche Verbesserungen und neuen Ansätzen führen wird.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2298: \nTheoretische Grundlagen von Deep LearningInternationaler BezugFrankreich, Großbritannien, Kanada, ÖsterreichKooperationspartnerDr. Jörg  Bornschein;Dr. Zhenwen  Dai;Georgios  Exarchakis, Ph.D.;James  Lucas"
    }
  ]
}