{
  "spp_number": "SPP 2199",
  "spp_title": "Skalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
  "spp_url": "https://gepris.dfg.de/gepris/projekt/402731241",
  "projects_count": 18,
  "projects": [
    {
      "project_id": "425867974",
      "url": "https://gepris.dfg.de/gepris/projekt/425867974",
      "title": "Blick-unterstützte skalierbare Interaktionen in pervasiven Klassenräumen",
      "investigators": "Professorin Dr. Anke  Huckauf;Professor Dr. Enrico  Rukzio",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable ComputingAllgemeine, Kognitive und Mathematische Psychologie",
      "funding_period": "Förderung von 2020 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425867974",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Viele Vorhersagen über die Verschiebung von Interaktionsparadigmen, weg von der Interaktion mit dedizierten Einzelgeräten, hin zur Interaktion mit der Umgebung selbst, sind wahr geworden. Allerdings sind wir noch weit entfernt von einer nahtlosen Interaktion zwischen Benutzern und den pervasiven Umgebungen, in denen wir bereits leben. Eines der Hauptprobleme ist, dass die Mensch-Computer-Interaktion oft noch immer als die Interaktion zwischen einem Benutzer und einem Gerät, anstatt der kollaborativen multimodalen Interaktion vieler Benutzer mit vielen Geräten gesehen wird. Die jüngsten Fortschritte im mobilen Eyetracking ermöglichen heute die Integration von Informationen, die basierend auf Verhaltensmustern der Augen abgeleitet werden können, etwa mögliche Absichten oder die aktuelle Kontext-Situation der Nutzer. Angesichts des Potenzials dieser Technologie ist es eines der Hauptziele des Projekts, neuartige, blickunterstützte, skalierbare Interaktionsparadigmen für pervasive Umgebungen zu entwerfen, zu entwickeln und zu bewerten. Ein prominentes Beispiel für pervasive Umgebungen sind Klassenzimmer, die ein enormes Potenzial für kollaborative Interaktion bieten. Dies wird derzeit aufgrund der begrenzten geräteübergreifenden und Mehrbenutzer-Interaktionsparadigmen nicht genutzt. Wir wählen deshalb öffentliche Klassenzimmer als Referenzszenario für das geplante Projekt, da es ein hervorragendes Testfeld für blickunterstützte, skalierbare Interaktion bietet. Wir werden uns auf mehrere Herausforderungen konzentrieren, die in Klassenzimmern vorhanden sind. Insbesondere werden wir betrachten, wie sich mehrere Geräte und mehrere Benutzer in verschiedenen Rollen auf die Interaktion auswirken. Wir planen ein \"Live-Klassenzimmer\" einzurichten, welches es uns ermöglicht, das Benutzerverhalten in einem uneingeschränkten realen Szenario zu beurteilen. Hierbei werden wir aus zwei Perspektiven vorgehen: (1) Wie können spezifische augenbasierte Indikatoren definiert werden, die es ermöglichen, mentale Zustände der Benutzer während der Interaktion zu bewerten, und (2) Wie können effektive, effiziente und zufriedenstellende, skalierbare, blickbasierte Interaktionstechniken entwickelt werden? Hierzu werden wir zwei Langzeitstudien durchführen: eine zu Beginn zur Beurteilung des uneingeschränkten Nutzerverhaltens bei der Interaktion mit derzeit verfügbaren Interaktionsgeräten und die zweite am Ende des Projekts zur Bewertung von Interaktionstechniken, die während des Projekts entwickelt wurden. Die mit verschiedenen Methoden gewonnenen Erkenntnisse werden in eine kohärente Beurteilung skalierbarer Interaktionsparadigmen integriert. Dies wird ein gemeinsames Verständnis der mit Methoden verschiedener Disziplinen erzielten Ergebnisse sicherstellen und zeigen, wie deren Weiterentwicklung innerhalb des Projekts für weitere Forschung genutzt werden kann.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "425869382",
      "url": "https://gepris.dfg.de/gepris/projekt/425869382",
      "title": "Design and Evaluation skalierbarer verhaltensbiometrischer Systeme in Pervasive Computing Umgebungen",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2020 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425869382",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Knowledge-based authentication mechanisms that require users to remember login and password are still among the most popular means for authentication. The average user is expected to access sensitive information through about 200 different, password-protected accounts in 2020. The caveat of such mechanisms is that they require people to use more passwords than they can remember and that entering passwords requires a significant amount of time. The number of authentications will further increase as more and more pervasive computing devices are being used in smart public spaces. These devices will not be recognized as computing devices anymore and may use predominantly interaction techniques most likely not suitable for entering knowledge-based passwords (e.g., gestures, speech). Examples include personal devices (e.g., smart glasses, smart phones, HMDs, smart clothes, wearables) as well as devices in the environment (interactive displays, pressure-sensitive floors, etc.) and will jointly create a pervasive computing environment. In past years, behavioral biometrics, that is the ability to identify users implicitly from their behavior, received considerable attention in the research community. This approach does not require users to remember a secret but authentication can seamlessly slide into the background. In particular, researchers showed that numerous behavior traits (gait, typing behavior, touch targeting, gaze) can be used for identification. At the same time, behavioral biometrics so far was mainly investigated in the lab for single users, since assessing different features’ biometric value requires precise measurements. Thus, it remains unclear how these approaches scale to novel challenges of pervasive computing environments.In this project, we examine how pervasive computing environments can leverage behavioral biometrics for identifying and authenticating users. The main challenge this project is addressing is the question how behavioral biometric approaches scale to different pervasive computing environments, containing multiple users with changing behavior, different physicalities, and changing sensing and interaction capabilities. From this objective, many questions emerge: (1) How is users’ behavior influenced both by other people in the vicinity, characteristics of a space, as well as by novel interaction techniques emerging as more and more computers become part of our everyday life; (2) how does this influence the way in which we design and develop behavioral biometrics; and (3) what does this mean for behavioral biometrics-based authentication concepts.We envision this project to enable a significant leap forward towards behavioral biometrics becoming a powerful means for identifying and authenticating users in future pervasive computing environments that combines high usability with strong security. The project outcomes are valuable beyond security, enabling novel UIs to be built that adapt to users’ behavior.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "425868555",
      "url": "https://gepris.dfg.de/gepris/projekt/425868555",
      "title": "Gesteninteraktionsparadigmen für Intelligente Umgebungen",
      "investigators": "Professorin Dr. Susanne  Boll;Professor Dr. Antonio  Krüger",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2020 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425868555",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Freihandgesten haben ein großes Potenzial, die Interaktion mit intelligenten Computerumgebungen zu gestalten und könnten neben dem Touch-Paradigma ein weiteres wichtiges Interaktionsparadigma in der Zukunft darstellen. Während die 3D-Gesteninteraktion seit vielen Jahren erforscht wird, sind die wissenschaftlichen Erkenntnisse über ihre syntaktische Struktur und die Anwendbarkeit in verschiedenen intelligenten Computerumgebungen immer noch fragmentiert und an bestimmte begrenzte Domänen gebunden. Dies macht es für Interaktionsdesigner von intelligenten Computerumgebungen sehr schwierig zu entscheiden, welche spezifischen Gesten in welchen Situationen und Umgebungen verwendet werden sollten. Basierend auf der Erkenntnis, dass robuste Techniken zur Erkennung von Gesten auf Verhaltensebene bald überall verfügbar sein werden (durch verschiedene Sensoren, z.B. Tiefenkameras), wollen wir Modelle entwickeln und auswerten, die alle relevanten syntaktischen und semantischen Aspekte von Gesten abdecken, die für die erfolgreiche Anwendung von Gesten in intelligenten Computerumgebungen von Bedeutung sind. Zu diesem Zweck planen wir die Entwicklung eines allgemeinen Gestenvokabulars und einer Gestengrammatik, die über Domänen hinweg einsetzbar ist. Diese werden auf einer groß angelegten Erhebung in zwei Domänen basieren: Smart Homes Bereich und intelligente Einzelhandelsumgebungen, die zu einem großen Gestenkorpus für weitere Analysen zusammengefasst werden. Eine neuartige Gestennotation wird entwickelt, um die Annotation des Korpus zu erleichtern und Freihandgesten für weitere Auswertungszwecke zu reproduzieren. Das geplante Gestenmodell basiert dabei auf vier Dimensionen, die sich auf die sozialen, räumlichen, physischen und kognitiven Anforderungen im Zusammenhang mit Freihand-Gesten beziehen. Im Laufe des Projekts werden entsprechende Metriken entwickelt, um einen aussagekräftigen Vergleich von Gesten zu ermöglichen. Diese Metriken werden auch dazu beitragen, den Korpus umfassender als bisher möglich zu annotieren. Unsere Ergebnisse werden in groß angelegten Feld- und Laborstudien empirisch validiert. Als weiteres Ergebnis des Projekts planen wir die Bereitstellung einer Software-Suite, die es anderen Forschern ermöglicht, unsere Studien durchzuführen und zu replizieren. Während des Projekts werden wir einzelne Module der Software-Suite entwickeln und sukzessive nutzen und unseremkommentierten Gestenkorpus zur Verfügung stellen. Dazu gehören die Entwicklung eines Gestennotation-Editors, der zur Bearbeitung des Corpus verwendet wird, eines Annotation-Editors, der die Kommentierung der Geste entlang der vier oben beschriebenen Dimensionen erlaubt, eines Grammatik-Editors, der Semantik für Gestensequenzen bereitstellt, und eines Evaluierungs-Toolkits zur Unterstützung von Online-Nutzungsstudien mit einem Gestenkorpus. Die Software-Suite wird im Laufe des Projekts sukzessive der Allgemeinheit zur Verfügung gestellt.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "425869442",
      "url": "https://gepris.dfg.de/gepris/projekt/425869442",
      "title": "Illusionäre Oberflächenschnittstellen",
      "investigators": "Professor Dr. Albrecht  Schmidt;Professorin Dr.-Ing. Katrin  Wolf",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2020 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425869442",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Weisers Vision nach verschwinden Computer künftig und werden sich in das „Gewebe des Alltags einweben“, bis sie nicht mehr davon zu unterscheiden sind. Dies stellt das Schnittstellendesign pervasiver Computer vor neue Herausforderungen. Herkömmliche Computer nutzen v.a. visuelle Möglichkeiten, um Funktionen und Interaktionsmöglichkeiten der Schnittstellen zu kommunizieren. Eine solche reichhaltige Informationsvisualisierung entspricht jedoch nicht der Prognose, wie allgegenwärtige Computer gestaltet sein werden. In diesem Projekt wollen wir multisensorische Illusionen beim Berühren der Oberflächen intelligenter Objekte auslösen und damit den Gestaltungsraum der Schnittstellen pervasiver Computer erweitern. Wie bei der Gummihand-Illusion, bei der die Menschen das fühlen, was sie sehen, werden wir visuelles und haptisches Feedback unterstützen. Statt das Design unserer Alltagsobjekte zu ändern, werden wir sie zum einen mithilfe intelligenter Brillen und Projektion visuell und zum anderen durch multisensorische Illusionen haptisch augmentieren. Illusionäres Feedback beim Berühren einer Objektoberfläche biete großes Potenzial, dem pervasiven Interaktionsdesign Materialität hinzuzufügen. Glatte und harte Oberflächen könnten als weich, uneben, flexibel oder verformbar wahrgenommen werden. Dies ermöglicht, den Mangel an visuellen Möglichkeiten, den Alltagsobjekte haben, durch illusorisch Multimodalität zu kompensieren. Im Rahmen von Nutzerstudien untersuchen wir, wie ein solch multisensorisches Interaktionsparadigma für Alltagsgegenstände aussehen sollte. Wir werden zunächst Parameter, die beim Berühren von Oberflächen weicher, dehnbarer und verformbarer Materialien hervorgerufen werden extrahieren, um die multisensorische Oberflächenwahrnehmung zu beschreiben. Wir werden dann die analoge multimodale Erfahrung mit multisensorischer Illusion nachahmen. Im nächsten Schritt werden wir untersuchen, wie die Parameter der Illusion angepasst werden können, um die multisensorische Illusion gezielt zu „programmieren“. Schließlich werden wir illusionäre Oberflächeninteraktion für die gesamte Buxton-Taxonomie von Eingabegeräten entwerfen, implementieren und evaluieren. Durch die Kombination des komplementären Fachwissens in den Bereichen computerbasierte Wahrnehmungsverstärkung, Ubicomp und Interaktionsdesign werden wir ein skalierbares, vereinheitlichtes und experimentell validiertes Modell für nutzungsfreundliche berührungsbasierte Schnittstellen mittels sensorischer Illusion erstellen. Wir werden ferner einen technologischen Rahmen für dessen effiziente Implementierung bereitstellen, um einen problemlosen Einsatz in intelligenten persönlichen Räumen und in Kontrollräumen zu ermöglichen. Dieses Modell basiert auf der Theorie der multisensorischen Integration und bildet den theoretischen Transfer von der Kognitionswissenschaft hin zu einem neuen Interaktionsparadigma, das eine umfassende und skalierbare Interaktion zwischen Mensch und Computer ermöglicht.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "521602817",
      "url": "https://gepris.dfg.de/gepris/projekt/521602817",
      "title": "Kompensation intersensorischen Unbehagens in Echtzeit",
      "investigators": "Professor Dr. Albrecht  Schmidt;Professorin Dr.-Ing. Katrin  Wolf",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2023",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 521602817",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Sensorische Illusionen sind die Basis für glaubwürdige Mixed-Reality-Erlebnisse. Visuelle Illusionen, wie animierte Bilder, sind bereits gut erforscht. Die für eine realistischere Verschmelzung von physischen und virtuellen Eindrücken essenziellen Illusionen funktionieren jedoch oft nur unzureichend und können bei vielen Menschen Unwohlsein hervorrufen. Multimodale Illusionen treten auf, wenn Sinnesmodalitäten widersprüchliche Informationen liefern und eine Sinnesmodalität die wahrgenommene Information durch eine andere überschreibt, sodass der Gesamteindruck stimmig erscheint. Bisher ist noch wenig erforscht, wie multimodale Illusionen eine überzeugende und unbehagen-freie Mixed-Reality-Erfahrung schaffen können. Mehrere Forschungsprojekte, einschließlich unserer früheren Arbeiten, haben die Machbarkeit der technischen Umsetzung solcher Illusionen anhand verschiedener Einzelphänomene demonstriert. In diesem Projekt wollen wir multimodale Integration in Mixed-Reality systematisch erforschen. Zentraler Aspekt ist hier das Phänomen des Unwohlseins in Mixed-Reality, auch Cybersickness genannt. Ist die multisensorische Information inkohärent, reagiert der Mensch darauf. Wir fühlen wir bspw. unwohl. Reisekrankheit und Cyberkrankheit sind Phänomene, bei denen diese Diskrepanz zwischen gefühlter und gesehener Bewegung nicht in eine kohärente Wahrnehmung integriert werden konnte. Unser Bestreben ist, physiologische Messungen zu nutzen, um den Beginn einer solchen Diskrepanz zu erkennen, bevor sich die Menschen unwohl fühlen. Wenn dies möglich ist, könnten wir die Diskrepanz korrigieren und eine funktionierende Illusion in Mixed-Reality erzeugen. Hierbei beziehen wir uns auf Forschungen, die untersucht haben, unter welchen Bedingungen intersensorische Integration technisch realisiert werden kann. Dies soll als Basis dienen, systematisch konzeptionelle Modelle dafür zu erstellen, welche sensorische Informationskombination welche sensorische Illusion erzeugen kann. Wir erweitern bestehende statischen Modelle um physiologische Wahrnehmung, um intra- und interpersonelle Unterschiede, die kognitive Modellen zu eigen sind, zu überwinden. Unsere Vision ist es, die wissenschaftlichen Grundlagen für eine neue Generation von Mixed-Reality-Systemen und -Anwendungen zu ermöglichen, die das Stören einer Illusion detektieren und ihm entgegenwirken können. Wenn wir in Echtzeit messen können, wann ein Benutzer eines interaktiven Systems multisensorische Informationen nicht mehr integrieren kann, könnte das System die multisensorische Ausgabe anpassen und Unwohlsein vermeiden. Ein Beispiel hierfür ist die Anpassung der visuellen Szene, sobald erkannt wird, dass eine Illusion nicht mehr funktioniert. Hierdurch könnten MR-Technologien für mehr Menschen und Anwendungen genutzt werden, und somit ein neuartiges Interaktionsparadigma durch Intersensory Discomfort Compensation in Echtzeit geschaffen werden.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "427133456",
      "url": "https://gepris.dfg.de/gepris/projekt/427133456",
      "title": "Koordinationsfonds",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2020",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 427133456",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "The core research question ahead of us is how to make the paradigms of interaction scale to large and complex pervasive computing environments. In the previous section, we introduced exemplary application domains to illustrate the challenges for interaction paradigms in pervasive computing environments and set the leading goal of this Priority Programme. In this section, we will identify three core research areas that we aim to pursue with this program. First and foremost, we aim to study interaction paradigms that are scalable in the sense of overarching large ensembles of interactive devices in pervasive computing environments. We need to investigate how we design and evaluate methods that span devices and physical entities easily (1). However, to evaluate novel interaction paradigms in and across settings, we need to adapt our methods for user studies in pervasive computing environments to give us robust results for experiments in the wild (2) and assess the newapproaches quantitatively (3). 1. Design of efficient and meaningful scalable interaction paradigms for pervasive computing environments: How do existing interaction paradigms scale to pervasive computing environments, i.e., distributed ensembles computational devices? What are the characteristics of interaction paradigms that can be used across devices and domains? How can we ensure that interaction paradigms can be used independently of the context but still consider the context-induced restrictions? Are there fundamental limitations that prevent the adoption of a single pervasive interaction paradigm? How do we address issues of efficiency as well as broader aspects of meaning through these interaction paradigms? 2. Rigorous and robust evaluation of scalable interaction paradigms in pervasive computing environments: How do we evaluate interaction techniques that are supposed to work across a range of devices and domains? Can there be standardized study methods to evaluate interaction paradigms for pervasive computing environments? What are the methods to evaluate interaction paradigms in-situ? How far can we extend unsupervised observation techniques by modern sensor technology to reach a reliable understanding of the usage of pervasive computing environments? Can model-based simulation of user interaction speed up the design phase and enable us to select promising interaction designs early in the design process? 3. Assessment of the success of interaction paradigms in pervasive computing environments: What are the metrics that measure and describe actual success, effectiveness, and satisfaction in large settings of pervasive computing environments? What are the score and value under which we rate a design effective and efficient but also meaningful and pleasant for an individual? What is a good balance between traditional performance metrics such as task performance and error rate versus user experience, joy of use, and well-being? What are meaningful testbeds to verify the resultsDFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "521603563",
      "url": "https://gepris.dfg.de/gepris/projekt/521603563",
      "title": "Manipulation der virtuellen Selbstwahrnehmung durch visuell-haptische Avatar-Parameter",
      "investigators": "Professor Dr. Nils  Henze;Professorin Dr.-Ing. Katrin  Wolf",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2023",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 521603563",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Benutzerrepräsentationen in interaktiven Systemen sind essenziell für die Entwicklung effektiver, effizienter und zufriedenstellender Werkzeuge. In Mixed Reality wird diese Repräsentation als Avatar bezeichnet und beschreibt virtueller Charaktere, durch die Benutzer:innen interagieren. Visuell-motorisch Synchronisierung von Avatar und Nutzer:in erzeugt das Gefühl, den Avatar zu verkörpern und ihn als eigenen Körper zu akzeptieren. Das macht Avatare zu einem vielversprechenden Werkzeug für viele Anwendungen, wie Angsttherapie, Behandlung von Körperbildstörungen oder Feuerwehrtraining, da Menschen in einer Simulation mit Herausforderungen konfrontiert werden können, ohne sie in Gefahr zu bringen. Frühere Arbeiten zeigen, dass Avatare Verhalten, Einstellung und Wahrnehmung von Benutzer:innen verändern können. Benutzer:innen zeigen beispielsweise eine höhere kognitive Leistung, wenn sie einen Avatar verkörpern, der wie Albert Einstein aussieht, und nehmen die Welt mit den Augen eines Kindes wahr, wenn sie eine niedrige Perspektive einnehmen. Während sich frühere Arbeiten auf die Effekte der audiovisuellen Darstellung von Avataren fokussierten, gewann Haptik erst in letzter Zeit an Bedeutung. Aktuelle Arbeiten zeigen, beispielsweise, dass das Heben von Gewichten als einfacher empfunden wird, wenn muskulöse Avatare verkörpert werden. Obwohl Haptik die Immersion steigern könnte und neue Möglichkeiten für Anwendungen, wie Psychotherapie oder zum Erlernen manueller und Bewegungsfertigkeiten eröffnen würde, sind Grundlagen zu visuell-haptischem Avatar-Design und dessen Auswirkung auf die Selbstwahrnehmung weitestgehend unerforscht. In diesem Projekt wird die Wirkung der visuellen Darstellung von Avataren in Kombination mit haptischen Reizen auf die Selbstwahrnehmung von Benutzern systematisch untersucht. Nutzer:innen nehmen die Welt nicht nur aus einem anderen Körper wahr, sondern fühlen sie auch entsprechend. Dies erhöht die Immersion, hilft sich in Avatare einzufühlen, ermöglicht neue Mixed-Reality-Anwendungen und erweitert unser Verständnis der zugrunde liegenden Phänomene. In empirischen Studien untersuchen wir die Auswirkungen haptisch-visuell präsentierter Avatare auf die Selbstwahrnehmung. Wir ermitteln, wie sich das visuelle Erscheinungsbild des Avatars auf die wahrgenommene Kraft, Ausdauer und Körperform, auswirkt. Ein muskulöser Avatar könnte beispielsweise dazu führen, dass Gewichte als leichter empfunden werden. Zusätzlich integrieren wir haptische Feedback-Geräte (wie Exoskelette und gewichtsverändernde Geräte), die die körperliche Leistung dynamisch beeinflussen, z. B. durch Manipulation der Anstrengung beim Heben von Gewichten. Wir kombinieren beide Forschungsrichtungen, um die Interaktion von Avatar-Erscheinungsbild und haptischen Feedback zu untersuchen, systematische Effekte aufzuzeigen und in ein Modell zu überführen, das die Wechselwirkung von visuellen und haptischen Stimuli auf die virtuelle Selbstwahrnehmung beschreibt.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "425868361",
      "url": "https://gepris.dfg.de/gepris/projekt/425868361",
      "title": "Mehr als Sicherheit und Effizienz in der Akutmedizin: Die Erfahrung einer verkörperten Personal-Umwelt Interaktion",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2020 bis 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425868361",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Technische Entwicklungen, Evaluationen und Forschung fokussieren sich in komplexen, sozio-technischen Domänen, wie Kraftwerken und Krankenhäusern, auf eine sichere und effiziente Mensch-Technologie-Interaktion. Implizit oder explizit wurden in der Forschung dazu nur Interaktionskonzepte berücksichtigt, die auf klassischen HCI Theorien wie z.B. Interaktion als Informationsaustausch (maximaler Durchsatz von Information) beruhen. Unsere Vorarbeiten zeigten jedoch, dass Interaktionskonzepte, die auf modernen HCI Theorien basieren, in der Akutmedizin ein größeres Erklärungspotenzial haben. Des Weiteren beachten klassische Theorien nicht das Erleben der Nutzer (user experience) in der Interaktion. Außerdem begrenzt sich die Interaktion in der Akutmedizin auf einzelne, separate Geräte und eine Vernetzung wird nur für Datendokumentation genutzt und nicht für eine pervasive Interaktion von Personal und Umwelt.Wir untersuchen wie das Personal die Mensch-Technik-Interaktion in der Akutmedizin wahrnimmt indem wir qualitative Daten sammeln und in den Zusammenhang mit theoretischen Konzepten über Interaktion setzen. Die Daten werden sowohl anhand von induktiv (aus den Daten) erarbeiteten Kategorien als auch anhand von deduktiv (aus den Theorien über Interaktion) erarbeiteten Kategorien analysiert. Des Weiteren untersuchen wir anhand der qualitativen Daten und Umfragen die Nützlichkeit von verschiedenen Konzepten über user experience wie instrumentelles, eudämonistisches und hedonisches Erleben. Die Erkenntnisse werden genutzt um eine erste Version eines Fragebogens über user experience in sicherheitskritischen Domänen zu erstellen.Wir designen und evaluieren basierend auf dem Ansatz der verkörperten Kognition (embodied cognition) eine pervasive Personal-Umwelt-Interaktion. Anhand von jüngsten embodied cognition Designprinzipien, unseren Erkenntnissen aus dem Feld und technologischen Möglichkeiten konzipieren wir mit Nutzern Prototypen. Diese werden in einer realen medizinischen Simulationsumgebung soweit umgesetzt, dass die Nutzer die Interaktion mit der Umgebung für spezifische Aufgaben erleben können. Wir werden die Interaktion in Bezug auf die technische Interaktion, die user experience (anhand des Fragebogens, der dabei weiterentwickelt und validiert wird) und Sicherheit iterativ evaluieren. Die vielversprechendste Interaktionstechnik wird für eine komplette und repräsentative Simulation genutzt. Zusätzlich zu den oben genannten Maßen erlaubt diese Simulation dann auch Variablen wie Teamarbeit und Situationsbewusstsein zu messen, die nur in dynamischen und repräsentativen Situation auftreten.Das Projekt trägt dazu bei, das Erklärungspotential und das Designpotential moderner theoretischer Konzepte über Interaktion im Rahmen von sicherheitskritischen Domänen zu bestimmen. Die Ergebnisse tragen zum Verständnis und Ausbalancieren von Effizienz und Bedeutsamkeit in Bezug auf eine pervasive Personal-Umwelt-Interaktion unter Wahrung der Sicherheit bei.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.MitverantwortlichDr. Oliver  Happel"
    },
    {
      "project_id": "425868829",
      "url": "https://gepris.dfg.de/gepris/projekt/425868829",
      "title": "PervaSafe Computing: Musterbasierte tragbare Assistenten für sicherheitskritische Mensch-Computer-Interaktion in Leitwarten",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2020 bis 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425868829",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Leitwarten sind wichtige Elemente moderner Gesellschaften, denn sie gewährleisten die Sicherheit und das Wohlbefinden von Menschen in vielen Situationen, sei es beim Einsatz von Rettungswagen, in der Verkehrsführung oder bei der Versorgung mit Strom, Gas und Wasser. Die Operateure tragen dabei große Verantwortung und sind auf passende Hilfsmittel angewiesen. Während sich Informations- und Kommunikationstechnologien in Leitwarten in den letzten 30 Jahr stark verändert haben, sind die Benutzungsschnittstellen nach wie vor hauptsächlich mit stationären Endgeräten und Anwendungen im WIMP-Paradigma-Stil verbunden. Fähigkeiten und Bedürfnissen der Operateure wird zu wenig Aufmerksamkeit geschenkt.Die Ziele des Projektes decken die Research Area 1 und 2 der Ausschreibung ab und verknüpfen diese. Im Detail wollen wir: - in einem menschzentrierten und partizipativen Gestaltungsprozess Entwurfsmuster und eine Mustersprache für skalierbares Interaktionsdesign in Leitwarten entwickeln. Die Entwurfsmuster werden von den Tätigkeiten, Arbeitsabläufen und Bedürfnissen der Operateure abgeleitet. Dabei werden Normal- und Ausnahmebetrieb, Automatisierungsstufen sowie individuelles und kooperatives Arbeiten berücksichtigt (Research Area 1).- ein Rahmenkonzept für durch Leitwartenmitarbeiter zu tragende computerbasierte Assistenten entwickeln, um zu erforschen, wie solche Systeme genutzt werden können, um einerseits die zuvor genannten Entwurfsmuster zu realisieren und andererseits ihre Nutzung vor Ort und unbeobachtet zu evaluieren (Research Area 1 und 2). Teil dieser Studien ist die Abbildung kognitiver Belastung und affektiver Zustände (z.B. Stress) von Leitwartenmitarbeitern mithilfe eines tragbaren Computersystems. Es wird genutzt, um Informationsflüsse, Alarmmeldungen und andere Ereignisse adäquat zu vermitteln (Research Area 1 und 2). Leitwartenmitarbeiter werden bei der Aufgabenerledigung unterstützt, indem spezifische Arbeitsschritte erkannt werden.- die Entwurfsmuster und ihre Umsetzung mithilfe des tragbaren Assistenten unter möglichst realistischen, aber reproduzieren Bedingungen mit Leitwartenmitarbeitern zu validieren und zu evaluieren. Gebrauchstauglichkeit und Nutzungserlebnis sind dabei von besonderer Bedeutung (Research Area 2). Formative und summative Evaluationsmaßnahmen werden umgesetzt. Die Forschung zur User Experience wird geleitet von der Frage: Empfinden Leitwartenmitarbeiter einen tragbaren computerbasierten Assistenten als Bevormundung (im Sinne der Autonomie und Kompetenz) oder als Unterstützung (im Sinne der Sicherheit). Mit Blick auf die Gebrauchstauglichkeit werden expertenbasierte Verfahren und Nutzerstudien durchgeführt. Drei Szenarien prägen dabei den Gesamtprozess (Netzleitwarte, Rettungsleitwarte, Schiffsbrücke als mobile Leitwarte).Das Projekt trägt dazu bei, besser zu verstehen, welche Interaktionsformen für Leitwarten und sicherheitskritische Pervasive-Computing-Umgebungen allgemein (PervaSafe Computing) geeignet sind.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "521584557",
      "url": "https://gepris.dfg.de/gepris/projekt/521584557",
      "title": "PervaSafe Computing: Skalierung von Leitwarten von Orten zu Räumen der Kontrolle",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2023",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 521584557",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Ob ein Krankenwagen benötigt wird, der Verkehr geregelt werden muss oder eine ununterbrochene Versorgung mit Strom, Gas und Wasser gewährleistet werden muss, Leitwarten sind für die Sicherheit und das Wohlbefinden der Menschen in verschiedenen Lebensbereichen von entscheidender Bedeutung. Sie werden jedoch in der Regel als geschlossene Einheiten, oft sogar ohne Fenster, verstanden, in denen menschliche Operateure mit stationären und desktopbasierten Schnittstellen interagieren. Aus verschiedenen Gründen (z. B. intelligente Umgebungen mit Daten- und Interaktionsmöglichkeiten, bessere Zusammenarbeit zwischen Außendienstmitarbeitern und Operateuren, Bedarf an flexibleren Arbeitsmodellen) erscheint es angebracht, einen breiteren Blickwinkel und Forschungsansatz \"jenseits der Leitwarte\" zu wählen. Dieser Antrag skaliert die Forschung \"jenseits der Leitwarte\" auf verschiedene Weise. Wir zielen darauf ab, (1) reproduzierbare Studien zur Interaktion in Leitwarten zu skalieren, indem wir die Auswirkungen verschiedener Simulationen und Evaluierungssettings in Bezug auf Immersion, physiologische Reaktionen der Teilnehmer und subjektiven Realismus vergleichen und messen, (2) Interaktionen in der Leitwarte auf den Menschen zu skalieren, indem wir Funktionen einbeziehen, die es den Operateuren ermöglichen, ihren Aufmerksamkeitszustand selbst zu steuern, und (3) die Interaktionen der Operateure über die physischen Leitwarten hinaus zu skalieren, indem wir Interaktionsmöglichkeiten in Betracht ziehen, die auf der Zusammenarbeit mit Akteuren außerhalb der Leitwarte beruhen und Leitwarten als Räume und nicht als Orte der Kontrolle betrachten.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "521584224",
      "url": "https://gepris.dfg.de/gepris/projekt/521584224",
      "title": "PriMR — Gestaltung und Evaluation skalierbarer Nutzungsschnittstellen zur Kommunikation und Kontrolle von Datenschutz-Aspekten in Mixed Reality",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2023",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 521584224",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Mixed Reality (MR) Headsets ermöglichen zahlreiche neue Anwendungen, unter anderem in den Bereichen Freizeit, Arbeit, Bildung und Marketing. Mit MR können die Nutzer in eine virtuelle Welt eintauchen oder ihre Sicht auf die reale Welt mit virtuellen Inhalten erweitern. Um dies zu erreichen, verwenden MR-Headsets eine Reihe von Sensoren, welche sensible Daten erfassen, verarbeiten und mit Dritten teilen können. Moderne Headsets ermöglichen den Zugriff auf Verhaltensdaten (Hand- und Körperbewegungen, Blick), physiologische Daten (EEG, Herzfrequenz), und Kontextdaten (Tracking-Raum, Passive Nutzer). Aus solchen Daten lassen sich Informationen über Demografie, Gesundheitszustand und Behinderungen ableiten. Es ist offensichtlich, dass solche Daten sensibel sind. Zwar sind Sensoren erforderlich, um das Tracking und Interaktion zu ermöglichen, doch können die erfassten Daten missbraucht werden. Dies stellt eine Herausforderung dar, da der Zugang zu den Daten notwendig ist, um eine immersive Benutzererfahrung zu schaffen. Gleichzeitig ist es wichtig, aktive und passive Nutzer in die Lage zu versetzen, ihre Daten vor einer unbeabsichtigten Nutzung zu schützen. In diesem Projekt wird untersucht, wie Benutzerschnittstellen zur Kontrolle der Datenschutz für MR entwickelt werden können. Die Kern-Herausforderungen sind (1) wie aktive und passive Nutzer für die Auswirkungen der Nutzung von MR-Technologie auf die Privatsphäre sensibilisiert und (2) wie sie bei sinnvollen Entscheidungen bezüglich der Datenerfassung, -verarbeitung und -weitergabe unterstützt werden können. Da MR in zahlreichen Umgebungen genutzt, eine wachsende Zahl von Anwendungen (Spiele, Büro, Bildung) unterstützt wird, kontinuierlich neuartige Sensoren integriert sowie Nutzer mit unterschiedlichen Fähigkeiten einbezieht, stellen sich folgende Fragen: (1) Wie können MR-Benutzerschnittstellen das Bewusstsein dafür schärfen, welche Daten erhoben, verarbeitet und weitergegeben werden? (2) Wie kann das Datenschutzverhalten der Nutzer in realistischen Umgebungen untersucht werden? (3) Wie können passive Nutzer von MR-Nutzern über ein laufendes Tracking informiert werden und wie kann ihnen die Kontrolle über ihre Daten gewährt werden? (4) Wie können Benutzerschnittstellen die mit der Zustimmung zur Datenerhebung und -weitergabe verbundenen Datenschutzrisiken effizient und in geeigneten Momenten kommunizieren? (5) Wie können MR-Benutzerschnittstellen die effiziente Zustimmung zu Datenschutz unterstützen? (6) Wie können die Auswirkungen von MR-Datenschutzschnittstellen bewertet werden? (7) Wie können Forscher und Praktiker bei der datenschutzgerechten Gestaltung von MR-Anwendungen unterstützt werden? Das Projekt ist ein wichtiger Schritt um Datenschutz zu einem integralen Aspekt bei der Entwicklung von MR-Anwendungen zu machen. Die Ergebnisse dieses Projekts tragen dazu bei, Herausforderungen des Datenschutzes bei der Konzeption und Entwicklung neuer Funktionen und Technologien zu berücksichtigen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "425827565",
      "url": "https://gepris.dfg.de/gepris/projekt/425827565",
      "title": "Prinzipien ästhetischer Interaktionsgestaltung für Pervasive Computing Environments im öffentlichen Raum",
      "investigators": "Professorin Dr. Sarah  Diefenbach;Professor Dr. Marc  Hassenzahl",
      "subject_area": "Arbeitswissenschaft, Ergonomie, Mensch-Maschine-SystemeSozialpsychologie und Arbeits- und Organisationspsychologie",
      "funding_period": "Förderung von 2020 bis 2024",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425827565",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Pervasive Computing-Environments (PCEs), d.h. die Vernetzung von Technologien wie Smartphone, Notebook, Fernseher, Beleuchtungssystemen oder sogar Autos zu \"smarten Umgebungen\", werden langsam zum Teil unseres Alltags. Die Anwendungen reichen von Smart Home Umgebungen bis hin zu öffentlichen Smart Spaces, wie z.B. Smart Restaurants oder automatisierten Sicherheitskontrollen am Flughafen. Dieser Trend erfordert neue Interaktionsparadigmen, welche die besonderen Eigenschaften von PCEs berücksichtigen und eine qualitativ hochwertige Interaktion aus Effizienz- und Erfahrungsperspektive ermöglichen. Wie im Call for Proposals zum SPP 2199 ausgeführt, besteht eine zentrale Herausforderung in deren Skalierbarkeit sowie der Identifizierung von geräte- und domänenübergreifend verwendbaren Charakteristika, die gleichzeitig kontextspezifische Anforderungen berücksichtigen, und damit eine positive User Experience (UX) ermöglichen. Das vorliegende Forschungsvorhaben nähert sich dieser Aufgabe mit einem Fokus auf die \"Ästhetik der Interaktion\", d.h. eine Interaktion, die sich gut \"anfühlt\", da sie mit dem Kontext und relevanten psychologischen Bedürfnissen im Einklang steht. Wir untersuchen hierbei das Referenzszenario \"Public Smart Spaces\" in welchem eine sensible Gestaltung und die Berücksichtigung von Erlebnisqualitäten besonders relevant scheinen. Im Vergleich zu privaten Situationen, in denen Interaktion aus der Perspektive eines einzelnen Nutzers betrachtet und bewertet werden kann, implizieren öffentliche Kontexte potenzielle \"Zuschauer\" und damit spezifische soziale Dynamiken. Neben dem Erleben der eigenen Interaktion, beschäftigt Nutzer auch, welchen Eindruck sie dabei auf andere machen. Damit wird die soziale Akzeptanz für PCEs im öffentlichen Raum zum entscheidenden Kriterium. Anhand einer Reihe systematischer Studien werden wir psychologische Bedürfnisse und spezifische Anforderungen an Erlebnis und Interaktion im öffentlichen Raum identifizieren und Interaktionskonzepte für drei Anwendungsszenarien (Smart Restaurant, Flughafen, Bürgeramt) prototypen und evaluieren. Es folgt die Ableitung generalisierbarer Interaktionsparadigmen und Evaluationsstrategien. Unser Forschungsprogramm adressiert damit Fragen der SPP Research Area 1 (Designprinzipien) und Research Area 2 (Evaluation). Der vorliegende Ansatz bietet eine wichtige Erweiterung bisheriger Arbeiten im Bereich von PCEs sowie HCI und Interaktionsdesign im Allgemeinen. Arbeiten zur Ästhetik von Interaktion haben bislang weder komplexe Systeme wie PCEs noch die performativen Aspekte von Interaktion im öffentlichen Raum umfassend beleuchtet. Zudem leiten sich derzeitige Interaktionsparadigmen oft aus einer spezifischen Technologie oder Modalität (z.B. Touch, Gesten, Sprache) ab. Das vorliegende Vorhaben hingegen konzentriert sich auf die universelle Beschreibung von Interaktionsattributen und Zusammenhängen zu Erlebnisqualitäten, unabhängig von der spezifischen Technologie oder Modalität.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "521601028",
      "url": "https://gepris.dfg.de/gepris/projekt/521601028",
      "title": "Privatsphäre-wahrende Interaktion mit am Körper getragenen Computergeräten",
      "investigators": "Dr. Katharina  Krombholz;Professor Dr. Antonio  Krüger;Professor Dr. Jürgen  Steimle",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2023",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 521601028",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Neuartige, am Körper getragene Geräte bieten neue, skalierbare Benutzerschnittstellen, die intuitiver und direkter zu bedienen sind. Allerdings birgt die körpernahe Ein- und Ausgabe ernsthafte neue Risiken für die Privatsphäre der Nutzer: Die großen Hand- und Fingergesten, die typischerweise für die Eingabe verwendet werden, sind wesentlich anfälliger für Beobachtung durch Dritte als die etablierten Formen der Toucheingabe. Das gilt in noch größerem Maße für visuelle Ausgabe am Körper. Dies ist besonders problematisch, da am Körper getragene Geräte typischerweise bei mobilen Aktivitäten in nicht-privaten Umgebungen verwendet werden. Das primäre Ziel dieses Projekts ist es, einen Beitrag zur Skalierbarkeit von On-Body-Computing in öffentlichen Umgebungen zu leisten, indem Interaktionstechniken für die Eingabe und Ausgabe privater Informationen entwickelt werden, die eine verbesserte Widerstandsfähigkeit gegenüber Verletzungen der Privatsphäre bieten. Im Mittelpunkt unseres Ansatzes steht das Ziel, die einzigartigen Interaktionseigenschaften des menschlichen Körpers zu nutzen: hohe manuelle Geschicklichkeit, hohe taktile Sensibilität und eine große verfügbare Oberfläche für Ein- und Ausgabe, gepaart mit der Möglichkeit, Ein- und Ausgabe durch variable Körperhaltung flexibel abzuschirmen. Diese Eigenschaften können die Grundlage für neue körperbasierte Ein- und Ausgabetechniken bilden, die skalierbar und (praktisch) unbeobachtbar sind. Dieses Ziel ist bisher weitgehend unerforscht. Es ist sehr anspruchsvoll aufgrund der neuen und höchst unterschiedlichen Formen und Skalierungen von körpernahen Geräten sowie der neuartigen Formen multimodaler Ein- und Ausgabe. Diese werden durch die inhärente Komplexität sozialer Umgebungen, die jeweilige Proxemik und die Aufmerksamkeit von Nutzern und Umstehenden weiter erschwert. Um einen Design-Raum für die Interaktionen zu erstellen, werden wir die Privatsphäre von taktiler Eingabe, visueller und haptischer Ausgabe an verschiedenen Körperstellen empirisch untersuchen, abhängig von der Körperhaltung und den proxemischen Konfigurationen. Anschließend werden wir systematisch körperbezogene Eingabegesten sowie skalierbare Techniken für multimodale Interaktion konzipieren und implementieren, die die Privatsphäre in sozialen Umgebungen hinsichtlich eines generalisierten Bedrohungsmodells wahren. Wir verwenden hierbei Aufmerksamkeitsmodelle, die den menschlichen Körper beinhalten. Die neuen Interaktionstechniken werden empirisch mit Nutzern in realistischen Szenarien und im Labor evaluiert, um zu bewerten, wie ihre Eigenschaften die Benutzerfreundlichkeit, den Datenschutz und die Skalierbarkeit beeinflussen. Beides wird uns helfen, die interne und externe Validität unseres Ansatzes zu verstehen. Wir erwarten, dass die Ergebnisse dieses Projekts wesentlich dazu beitragen werden, die Grundlagen für skalierbare körperbasierte Interaktionen zu schaffen, die die Privatsphäre wahren.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "425869111",
      "url": "https://gepris.dfg.de/gepris/projekt/425869111",
      "title": "RIME: Reichhaltige interaktive Materialien für Alltagsgegenstände im Wohnumfeld",
      "investigators": "Professorin Dr. Susanne  Boll;Professor Dr. Jan  Borchers;Professor Dr. Jürgen  Steimle",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2020 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425869111",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Das Nutzererlebnis des „Smart Homes” ist in der Krise. Intelligente Geräte ziehen rapide in Haushalte ein, mit einer verwirrenden Zahl zu lernender eigener Nutzeroberflächen. Dies führt zu Benutzungsfehlern, Frustration und oft Aufgabe vielversprechender Lösungen. Eine Zusammenführung über sprach- oder gestengesteuerte Assistenten führt zu schwer explorierbaren Benutzerschnittstellen und sozial unangemessenen Situationen, und erreicht rasch seine Grenzen, z.B. bei der Spracheingabe kontinuierlicher Werte. Das Smartphone als Universalfernbedienung passt schlecht zu natürlichen Praktiken im Haushalt und limitiert unsere Hände wie alle Touchscreens auf das Antippen und Wischen mit 1–2 Fingern auf einer glatten Glasfläche. Dies wird den Fähigkeiten der menschlichen Hand nicht gerecht.Evolutionär sind haben wir mit unbelebten Objekten und Materialien stets durch unsere Hände interagiert. Sprache und Gestik haben sich speziell zur Interaktion mit anderen Lebewesen entwickelt. Dies erklärt das Unbehagen, wenn Nutzer heute zu Haushaltsobjekten sprechen oder gestikulieren sollen. Über Jahrmillionen haben unsere Hände ein reichhaltiges Vokabular an Griffen und die Fähigkeit zur Exploration per Berührung entwickelt, um eine Vielzahl an Material- und Objekteigenschaften zu erfahren und zu genießen. RIME (Reichhaltige interaktive Materialien für Alltagsgegenstände im Wohnumfeld) will dieses Potenzial nutzbar machen und dazu parametrisch skalierbare, reichhaltige touchbasierte Interaktionen mit unseren persönlichen intelligenten Umgebungen im und jenseits des künftigen Zuhauses entwickeln, evaluieren und über Metriken bewertbar machen.RIME begegnet dieser Herausforderung mit einem strukturierten Forschungsplan, der Grundlagenforschung mit modernen nutzerzentrierten Prozessen der Mensch-Computer-Interaktion verknüpft. Im Zentrum steht die Idee, Objekte im Haus mit interaktiven Oberflächen und Materialien zu versehen, die komplexe Toucheingaben erkennen und angemessen multimodal reagieren können. Wir beginnen mit der Analyse von Alltagspraktiken rund um die Berührung solcher Gegenstände im Labor und vor Ort, um Platzierungsmöglichkeiten von RIME-Komponenten zu ermitteln (WP1). Wir leiten daraus mögliche Interaktionstechniken ab und untersuchen ihre Entdeckbarkeit, Wege zur Unterscheidung von Alltags- und digitalem Gebrauch, Abbildungen von RIME-Komponenten zu ihrem Zielobjekt, und die Skalierbarkeit auf verschiedene Nutzer und Kontexte (WP2). Wir evaluieren, wie lokale und ambiente Rückmeldungen gegeben werden können (WP3), und schaffen die nötigen Grundlagen zur digitalen Fabrikation skalierbarer, parametrischer Modelle dehnbarer Oberflächensensoren und -materialien, die Berührung, Kraft, passive und aktive Verformung sowie andere haptische und visuelle Ausgaben integrieren (WP4). Wir validieren unsere Ergebnisse iterativ mit Nutzergruppen und integrieren sie in konzeptionelle Rahmenwerke und modellbasierte Richtlinien für künftige Entwurfswerkzeuge (WP5).DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "425867851",
      "url": "https://gepris.dfg.de/gepris/projekt/425867851",
      "title": "Scalable Pervasive Health Environments",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung von 2020 bis 2025",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425867851",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Den meisten Menschen ist heute bekannt, wie wichtig ein bewusster und gesunder Lebensstil ist. Dennoch scheitern viele Menschen daran, die Motivation aufzubringen und aufrechtzuerhalten, um ihren Lebensstil langfristig positiv zu verändern. Exergames helfen dabei, Benutzer*innen zu motivieren, ihre Übungen regelmäßig durchzuführen. Ein wirklich langfristiger Motivationseffekt ist bisher jedoch nicht belegt. Gleichzeitig werden digitale Technologien mit dem Potenzial, Motivation, Anleitung und Analyse bzgl. körperlicher Aktivitäten bereitzustellen, allgegenwärtig verfügbar. Bisher sind diese jedoch nicht verbunden und nutzen daher das synergetische Interaktionspotenzial bei Weitem nicht aus. Die Erschöpfung dieses Potenzials macht skalierbare Interaktionstechniken für pervasive Umgebungen mit Ensembles aus verschiedenen Ein- und Ausgabegeräten notwendig, wie sie in der Ausschreibung des Schwerpunktprogramms adressiert werden. Dieses Projekt wird pervasive Umgebungen erstellen und erforschen, um an dieser Stelle anzusetzen. Welche körperlichen Aktivitäten und Übungen für eine*n Benutzer*in gesundheitlich wichtig sind - und auch bevorzugt werden – hängt stark von persönlichen Eigenschaften und Umständen ab und kann sich im Laufe der Zeit stark ändern. Daher adressiert das erste der beiden Tandem-Projekte den Forschungsbereich 1 (“Designing Scalable Interaction Paradigms for Pervasive Environment”), indem ein modulares System untersucht wird, welches die Kombination verschiedener Arten von Sensoren (z. B. Fitness-Tracker oder Sensormatten) erlaubt und auch ältere Sportgeräte durch die Integration neuer Sensoren in „smarte“ Geräte verwandelt. Diese Sensoren können in existierende Exergames und „off-the-shelf“ Computerspiele integriert werden und ermöglichen durch die Anreicherung mit einem anpassbaren Feedbacksystem neue Formen der skalierbaren pervasiven multimodalen Interaktion und neue Maße für den Trainingsfortschritt. Das Design, die langfristige Evaluation und Verbesserung solcher System stellen auf Grund der Komplexität der Kombinationen von verschiedenen Geräten und Modalitäten große Herausforderungen dar. Daher adressiert das zweite Tandem-Projekt den Forschungsbereich 2 (“Methods to Study Interaction Paradigms in Pervasive Computing Environments”) indem untersucht wird, wie virtuelle und erweiterte Realität zusammen mit notwendigen Verfahren aus dem Bereich der Datenanalyse und des maschinellen Lernens eingesetzt werden können, um die datengetriebene und langfristige Evaluation solcher pervasiven Gesundheitsumgebungen zu verbessern.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "521585176",
      "url": "https://gepris.dfg.de/gepris/projekt/521585176",
      "title": "SimGest -- Simulation skalierbarer gestenbasierter Mensch-Maschine-Interaktion",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2023",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 521585176",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Gestensteuerung ist eine Art der Mensch-Computer-Interaktion, bei der kein Zeiger auf ein virtuelles Objekt bewegt wird, sondern die Bewegung Nutzender selbst die Interaktion darstellt. Heutzutage sind Oberflächengesten eine gängige Interaktionsmodalität auf Mobiltelefonen und großen interaktiven Whiteboards, aber mit den Fortschritten in der virtuellen und erweiterten Realität und der zunehmenden Verfügbarkeit von Head-Mounted Displays gewinnen räumliche Gesten immer mehr an Bedeutung. Insbesondere Anwendungsfälle, die ungewöhnliche Szenarien abdecken, bringen zusätzliche Einschränkungen für das Gestendesign mit sich und erfordern die Erstellung eines Interaktionskonzepts, das kontextuelle Faktoren und andere Aspekte berücksichtigt.  Softwareentwickler stehen bei der Entwicklung gestenbasierter Anwendungen jedoch vor einer Vielzahl von Herausforderungen. Gesten müssen erkennbar sein, d.h. die Bewegungen des Benutzers müssen mit bekannten Gesten abgeglichen werden und die Anwendung muss entsprechend reagieren. Gesten müssen robust sein, d. h. die Reaktion der Anwendung darf nicht durch Leistungsunterschiede zwischen verschiedenen Benutzern beeinträchtigt werden. Gesten müssen auch zum Gerät passen, wobei verschiedene Faktoren wie die Haltung des Geräts eine Rolle spielen. Vor allem aber müssen die Gesten zum Benutzer passen, d.h. Ergonomie, Einprägsamkeit und semantische Mehrdeutigkeit der Gesten sowie die motorischen Fähigkeiten müssen bei der Entwicklung berücksichtigt werden, da sie die individuelle User Experience stark beeinflussen. Und Gesten müssen skalieren und sich an den Kontext und die Umgebung des Benutzers anpassen, d.h. an die aktuelle Situation, in der sich der Benutzer befindet, z.B. soziales Umfeld, Standort oder aktuelle Aufgabe. Diesen Herausforderungen muss nicht nur mit geeigneten Entwicklungswerkzeugen begegnet werden: Sie erfordern vor allem ein intensives Testen der Anwendung im Allgemeinen und der Interaktionsmodalitäten im Besonderen. Zum Testen der Interaktion gehört es außerdem, Eingaben zu generieren und die Ausgaben zu untersuchen, d.h. reale Gesten auszuführen und zu prüfen, ob die Anwendung wie erwartet reagiert. Da das manuelle Testen zeit- und kostenaufwändig ist, ist eine Automatisierung der Tests wünschenswert, setzt aber Fähigkeiten zur Gestensimulation voraus, die nicht im notwendigen Umfang vorhanden sind. Da Gesten unscharf sind, muss die Gestensimulation Gesten generieren, die im Hinblick auf die Abweichungen, die sich aus verschiedenen Benutzergruppen und den oben genannten Aspekten ergeben, verzerrt sind. Und je weiter der Stand der Technik zu anspruchsvolleren Arten der gestenbasierten Interaktion fortschreitet, z. B. mit intelligenten Textilien oder anderen Geräten, desto mehr müssen die Testmöglichkeiten mit den Gesten skalieren. In diesem Projekt wollen wir durch empirische Studien die notwendigen Grundlagen für die Simulation von Gestenleistungen verschiedener Nutzergruppen schaffen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "521586817",
      "url": "https://gepris.dfg.de/gepris/projekt/521586817",
      "title": "SKIRIM: Selbstgesteuerte kinetische Interaktion mit reichhaltigen interaktiven Materialien",
      "investigators": "Professorin Dr. Susanne  Boll;Professor Dr. Jan  Borchers;Professor Dr. Jürgen  Steimle",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable Computing",
      "funding_period": "Förderung seit 2023",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 521586817",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Vor der Einführung von Smart Homes war der Toaster das Einzige, was sich plötzlich von selbst bewegt. Heute öffnen sich Jalousien und Geschirrspülertüren von selbst. Diese sind technisch begründet und bleiben im Umfeld der privaten Wohnung eher ungewohnt. SKIRIM erforscht, wie weiche, formverändernde Materialien effektive, aber unaufdringliche Benutzeroberflächen (UIs) schaffen können, die sich natürlich in den Alltag einfügen. SKIRIM geht somit den nächsten Schritt bei der Verschmelzung von Digitalem und Physischem im der privaten Wohnung und bewegt sich von passiven, interaktiven Alltagsoberflächen zu weichen, selbständig formverändernden Benutzeroberflächen, die ihre physische Form dynamisch verändern. Formverändernde Materialien sind zwar heute schon grundsätzlich verfügbar, aber Interaktionstechniken, Modelle ihrer Wirkung auf Menschen, Designtools, Herstellungsprozesse und UI-Designrichtlinien fehlen, insbesondere bei weichen, formverändernden SKIRIM-UIs, in der häuslichen Umgebung. Wir untersuchen in welchen Smart-Home-Szenarien solche Benutzeroberflächen sinnvoll sind, entwickeln Vorhersagemodelle für die Wirkung der Bewegungsparameter auf Nutzer, wie die Geschwindigkeit der Formänderung, erforschen neue digitale Herstellungsansätze und entwickeln und evaluieren Werkzeuge für Experten und Endnutzer, zum Entwurf und Konfiguration von SKIRIM-UIs. So kombinieren wir grundlegende Forschungsmethoden mit Research Through Design und benutzerzentrierten Ansätzen. Unsere Arbeitspakete kombinieren die Kompetenzen der drei Einrichtungen: Jedes weist einen ähnlichen methodischen Ansatz auf, an dem alle Einrichtungen beteiligt sind. Diese konzentriert sich aber auf immer komplexere Herausforderungen der SKIRIM-UIs: von einzelnen Bausteinen über die Zusammenstellung zu Benutzeroberflächen bis hin zu deren Zusammenführung mit Alltagsmaterialien. Auf diesem Weg verlagert sich der Schwerpunkt von Designern zu Endnutzern und von Labor- zu Praxisstudien. SKIRIM trägt auf folgende Weise zum SPP bei: Die Benutzeroberflächen versprechen eine gute Integration in die häusliche Umgebung; durch die Formveränderung können sie für verschiedene Nutzende, Aufgaben und Umgebungen außerhalb des Hauses skaliert werden; sie ermöglichen natürlichere Eingaben und die Forschung an SKIRIM kommt zur rechten Zeit, da sie sich der großen Herausforderung stellt, die weiche Formveränderung von einer Materialeigenschaft in ein skalierbares Interaktionsparadigma zu wandeln. Die PIs dieses Antrags haben mit über 20000 Zitierungen einen starken Hintergrund im Bereich der HCI und sind momentan Co-PIs in RIME. Ihre Kompetenzen ergänzen sich in idealer Weise: PI Steimle konzentriert sich auf Materialien und Herstellung, PI Borchers auf Interaktionstechniken, Designtools und prototypische empirische Studien und PI Boll auf Smart-Home-Umgebungen, multimodales Interface-Design, nutzerzentriertes Design und Akzeptanzstudien.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    },
    {
      "project_id": "425412993",
      "url": "https://gepris.dfg.de/gepris/projekt/425412993",
      "title": "TransforM – Transparenz für Maschinen in Persönlichen intelligenten Umgebungen",
      "investigators": "Professor Dr. Andreas  Butz;Professorin Dr. Sarah  Diefenbach",
      "subject_area": "Bild- und Sprachverarbeitung, Computergraphik und Visualisierung, Human Computer Interaction, Ubiquitous und Wearable ComputingSozialpsychologie und Arbeits- und Organisationspsychologie",
      "funding_period": "Förderung seit 2020",
      "project_identifier": "Deutsche Forschungsgemeinschaft (DFG) - Projektnummer 425412993",
      "dfg_procedure": "Schwerpunktprogramme",
      "parent_program": "SPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen.",
      "description": "Das Projekt TransforM ist ein Folgeprojekt des SPP-2199-Projekts PerforM, in dem Persönlichkeit als Interaktionsparadigma für robotische intelligente Räume untersucht wurde. PerforM hat eine sogenannte \"Raumintelligenz\" entwickelt, d.h. eine intelligente Wohnumgebung mit Roboterelementen und intelligenten Küchengeräten, die als kohärente Einheit angesprochen werden kann und eine gewisse Persönlichkeit gegenüber ihren Nutzern aufweist. Bei der Erforschung dieses Paradigmas in Nutzerstudien zeigte sich ein Konflikt zwischen dem Wunsch nach Transparenz einer solchen Umgebung (der Benutzer kann verstehen, was hinter den Kulissen vor sich geht) und dem Wunsch nach Unsichtbarkeit der Technik, gerade im eigenen zuhause (der Benutzer möchte, dass die Dinge einfach funktionieren und sich nicht mit allen Details beschäftigen, einer behaglichen Atmosphäre) gibt. Das richtige Maß an Transparenz ist demnach entscheidend für die Akzeptanz einer solchen Umgebung. Aufbauend auf Transparenzkonzepten aus dem Bereich Explainable AI und der zwischen-menschlichen Kommunikation wollen wir untersuchen, wie Transparenz in persönlichen intelligenten Umgebungen wie der Raumintelligenz und darüber hinaus konstruiert und kommuniziert werden kann. Hierfür wollen wir eine Sprache und Toolbox von Transparenzbausteinen entwerfen und diese in mehreren Nutzerstudien evaluieren. Unter anderem werden diese Studien Einblicke liefern, wie die Transparenzbausteine von Nutzern wahrgenommen werden, welche Beziehungen zwischen Transparenz und Vertrauen und Akzeptanz bestehen, und zu prüfen, ob es möglich ist, einen \"Transparenz-Sweetspot\" zu identifizieren. Darüber hinaus werden wir das Potenzial von \"Adaptable Transparency\" untersuchen, d.h. Anpassungen des Levels an Transparenz an individuelle Charakteristika und Vorlieben von Nutzern (z.B. technische Expertise, Persönlichkeitsunterschiede). Basierend auf diesen Studien werden wir generelle skalierbare Interaktionsparadigmen und Gestaltungsrichtlinien für zukünftige Smart Spaces extrahieren, die sicherstellen, dass diese weder zu intransparent noch zu komplex sind und - durch das richtige Maß an Transparenz - ein angemessenes, kalibriertes Maß an Vertrauen erzeugen.DFG-VerfahrenSchwerpunktprogrammeTeilprojekt zuSPP 2199: \nSkalierbare Interaktionsparadigmen für allgegenwärtige Rechnerumgebungen."
    }
  ]
}